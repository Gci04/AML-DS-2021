{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKvVwL8cBn16"
   },
   "source": [
    "### Reccurent Neural Networks (LSTM, GRU, ATTENTION, Transformer, BERT)\n",
    "\n",
    "```\n",
    "- Advanced Machine Learning, Innopolis University \n",
    "- Professor: Muhammad Fahim \n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "1. Simple and staked LSTM\n",
    "2. Transformer based models\n",
    "3. Homework 1 presentation\n",
    "4. Lab Task\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KSWjYWxHRVNa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preliminaries for processing the text\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dx8uw8WQRx1K"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2i0XH-2jyX-n"
   },
   "outputs": [],
   "source": [
    "!pip install wget \n",
    "import wget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdM4OJi4LeRz"
   },
   "outputs": [],
   "source": [
    "#Download and unzip dataset\n",
    "wget.download(\"http://alt.qcri.org/semeval2016/task6/data/uploads/stancedataset.zip\")\n",
    "\n",
    "!unzip stancedataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WvM_MHehRwGK"
   },
   "outputs": [],
   "source": [
    "#Read dataset to dataframe\n",
    "\n",
    "train_data = pd.read_csv(\"./StanceDataset/train.csv\", header=0, engine='python' ,encoding = \"latin-1\", usecols=[\"Tweet\",\"Target\"])\n",
    "test_data = pd.read_csv(\"./StanceDataset/test.csv\", header=0, engine='python' ,encoding = \"latin-1\", usecols=[\"Tweet\",\"Target\"])\n",
    "\n",
    "test_data.query(\"Target != 'Donald Trump'\",inplace=True)\n",
    "\n",
    "labels_keys = {value: i for i, (value, count) in enumerate(train_data.Target.value_counts().items())}\n",
    "\n",
    "train_data['Target'] = train_data['Target'].apply(lambda x: labels_keys.get(x))\n",
    "test_data['Target'] = test_data['Target'].apply(lambda x: labels_keys.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "niNoZ2dRDh8w"
   },
   "outputs": [],
   "source": [
    "#TODO: preprocess each an every sentence (tweet text)\n",
    "def clean_data(text):\n",
    "  return None\n",
    "\n",
    "train_data['Tweet'] = train_data['Tweet'].apply(clean_data)\n",
    "test_data['Tweet'] = test_data['Tweet'].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cAMIafLjCryV"
   },
   "outputs": [],
   "source": [
    "#tokenize and create Vocab\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "\n",
    "for _, row in train_data.iterrows():\n",
    "  counter.update(tokenizer(row[\"Tweet\"]))\n",
    "\n",
    "vocab = Vocab(counter,specials=(\"<pad>\",\"<unk>\"), min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7kLLyQvVJpWu"
   },
   "outputs": [],
   "source": [
    "# Do padding\n",
    "def data_process(raw_text_iter,max_len=64):\n",
    "  batch = []\n",
    "  for item in raw_text_iter:\n",
    "    res = [vocab[token] for token in tokenizer(item)]\n",
    "    if len(res) > max_len : \n",
    "      res = res[:max_len]\n",
    "    if len(res) < max_len :\n",
    "      res += ([vocab[\"<pad>\"]] * (max_len-len(res)))\n",
    "    batch.append(res)\n",
    "  pad_data = torch.tensor(batch, dtype=torch.long)\n",
    "  return pad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DBGXz8-4kq2t"
   },
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "embedding_size = 10\n",
    "n_classes = len(np.unique(train_data.Target.values))\n",
    "\n",
    "#Create Dataloader\n",
    "train_tensor = data_process(train_data.Tweet.values)\n",
    "tgts_tensor = torch.nn.functional.one_hot(torch.from_numpy(train_data.Target.values), n_classes) #torch.from_numpy(train_data.Target.values)\n",
    "\n",
    "dataset = TensorDataset(train_tensor, tgts_tensor)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP63bpU_nks6"
   },
   "source": [
    "## Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_splgvZEjfHi"
   },
   "outputs": [],
   "source": [
    "class SimpleLstm(nn.Module):\n",
    "  def __init__(self, embedding_dim ,vocab_size , hidden_dim=10, output_dim=1, n_layers=1):\n",
    "    super().__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers,batch_first = True)\n",
    "    \n",
    "    self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "      \n",
    "  def forward(self, x):\n",
    "    batch_size = x.size(0)\n",
    "    embedded = self.embedding(x)\n",
    "    outputs, (hidden, cell) = self.lstm_layer(embedded)\n",
    "    \n",
    "    pred = self.output_layer(hidden[-1])\n",
    "    return pred\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 64\n",
    "output_dim = len(np.unique(train_data['Target']))\n",
    "model = SimpleLstm(embedding_dim=embedding_size, vocab_size=vocab_size, hidden_dim=10,output_dim=output_dim).to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5MfaLQA1ibV"
   },
   "source": [
    "## Model summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYcxZQC-N_jF",
    "outputId": "390727e4-5bce-4b22-82d6-5600c3d22ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLstm(\n",
      "  (embedding): Embedding(10009, 64)\n",
      "  (lstm_layer): LSTM(64, 10, batch_first=True)\n",
      "  (output_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tm2WeGqSy1l"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "K3vNEhD9R9fL"
   },
   "outputs": [],
   "source": [
    "#TODO: Implement Model train function which will return epoch loss and accuracy\n",
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "  epoch_loss = 0\n",
    "  epoch_accuracy = 0\n",
    "\n",
    "  return epoch_loss / len(data_loader) , epoch_accuracy / len(data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6oGin2PUTre",
    "outputId": "8e75c127-55f3-4c76-9b3f-b0b4b32fe8c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.6765171545535306\n",
      "Loss : 0.5912118203319257\n"
     ]
    }
   ],
   "source": [
    "# Train loop \n",
    "criterion = torch.nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  train_loss, train_acc = train(model, loader, optimizer, criterion, device=device)\n",
    "  print(f\"Loss : {train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9CTu5oI-XfH"
   },
   "source": [
    "## Transformers & Bert\n",
    "\n",
    "Sentiment analysis task. We are going to use the [transformers library](https://github.com/huggingface/transformers) to get pre-trained transformers and use them as embedding layers. Its possible to implement from scratch. Bert is one of the popular transformer based models, <br>\n",
    "* **Name other transformer based state-of-the-art models** <br>\n",
    "\n",
    "The transformers library can be easily installed with pip <br>`pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0VHgjie-W6T"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)\n",
    "\n",
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Dz67tzqbYNtM"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_trim(sentence):\n",
    "  tokens = tokenizer.tokenize(sentence) \n",
    "  tokens = tokens[:max_input_length-2]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bRfq6igNYahn"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BertBasedSentiment(nn.Module):\n",
    "  def __init__(self, transform_based_model, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "    super().__init__()\n",
    "    self.transform_based_model = transform_based_model\n",
    "\n",
    "    self.embedding_dim = transform_based_model.config.to_dict()['hidden_size']\n",
    "    self.gru = nn.GRU(self.embedding_dim, hidden_dim, num_layers = n_layers,bidirectional = bidirectional,batch_first = True,dropout = 0 if n_layers < 2 else dropout)\n",
    "    self.output_layer = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "  def forward(self, text):\n",
    "    # First pass the input text through bert. The output of bert is like embedding Remember: Bert is set to not trainable mode\n",
    "    with torch.no_grad():\n",
    "      embed = self.transform_based_model(text)[0]\n",
    "      \n",
    "    _, hidden = self.gru(embed)\n",
    "\n",
    "    if self.gru.bidirectional:\n",
    "      hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "    else:\n",
    "      hidden = self.dropout(hidden[-1,:,:])\n",
    "      \n",
    "    return self.output_layer(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMAuEWLSYyCa"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "hidden_dim = 256\n",
    "out_dim = len(np.unique(train_data['Target']))\n",
    "bi_directional = True\n",
    "dropout_rate = 0.25\n",
    "n_layers = 2\n",
    "\n",
    "model = BertBasedSentiment(bert_model, hidden_dim, out_dim, n_layers, bi_directional, dropout_rate).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cjrS-EtbZH4R"
   },
   "outputs": [],
   "source": [
    "#Set all the bert weights non-trainable\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith('bert'):\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_vfQqElZNyh"
   },
   "source": [
    "## Train Bert model\n",
    "\n",
    "First, we will specify the algorithm to update the model we parameters in the training process - optimizer. The most common is stochastic gradient descent (SGD). Secondly, we will specify the loss calculation function which is selected based on the learning objective (regression, classification, ..). Lastly, all the specified algorithms must be placed in the same training device where the model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5bzdNyzlZQkC"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# TODO: Select the optimizer and loss function/criterion\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngRq4wJYZpny"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  train_loss, train_acc = train(model, loader, optimizer, criterion)\n",
    "  print(f\"Epoch: {epoch}, Loss :{train_loss}, Accuracy: {train_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j29kvtTFeYk"
   },
   "source": [
    "## Lab Task \n",
    "\n",
    "```\n",
    "1. Write a predict function that takes in a trained net, a plain tweet text and prints out a tweet topic (label).\n",
    "2. Add make bi-directional LSTM for the classification of tweet topic. (Modify the simple LSTM model example)\n",
    "3. Create a validation set from the training data and log the models loss (training and validation) on tensorboard\n",
    "4. Visialize simple LSTM and transformer based model (bert) perfomence using confussion matrix \n",
    "\n",
    "```\n",
    "\n",
    "<center>Don't forget to commit</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSUY6VFQs5I9"
   },
   "source": [
    "## References\n",
    "1. [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](http://www.kurious.pub/blog/Illustrated-Guide-to-LSTMs-and-GRUs-A-step-by-step-explanation-6)\n",
    "\n",
    "2. [BERT Explained: What You Need to Know About Google’s New Algorithm](https://www.searchenginejournal.com/bert-explained-what-you-need-to-know-about-googles-new-algorithm/337247/#close)\n",
    "\n",
    "3. [Understanding searches better than ever before](https://www.blog.google/products/search/search-language-understanding-bert/)\n",
    "\n",
    "4. [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n",
    "\n",
    "5. [How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/)\n",
    "\n",
    "6. [A deep dive into BERT: How BERT launched a rocket into natural language understanding](https://searchengineland.com/a-deep-dive-into-bert-how-bert-launched-a-rocket-into-natural-language-understanding-324522)\n",
    "\n",
    "7. [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n",
    "\n",
    "8. [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "9. [BERT Word Embeddings Tutorial](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)\n",
    "\n",
    "10. [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf)\n",
    "\n",
    "11. [Efficient multi-lingual language model fine-tuning](https://nlp.fast.ai/)\n",
    "\n",
    "12. [BERT Text Classification in 3 Lines of Code Using Keras](https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358)\n",
    "\n",
    "13. [QUASI-RECURRENT NEURAL NETWORKS](https://arxiv.org/pdf/1611.01576.pdf)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AML_Lab41.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
