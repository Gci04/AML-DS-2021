{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "average-extension",
   "metadata": {
    "id": "burning-productivity"
   },
   "source": [
    "## <center>Final Exam Lab\n",
    "```\n",
    "- Advanced Machine Learning, Innopolis University \n",
    "- Professor: Muhammad Fahim \n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "```\n",
    "Tasks:\n",
    "  1. Data Preprocessing (5 points)\n",
    "  2. Conditional Generative adversarial network definition (5 points)\n",
    "  3. Conditional Generative adversarial network training (10 points)\n",
    "  4. Text explainer implemetation using Lime or Shap (5 bonus points)\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-convenience",
   "metadata": {
    "id": "purple-diana"
   },
   "source": [
    "## The Dataset\n",
    "\n",
    "For this task the 20 newsgroups text dataset is used. [LINK](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-creek",
   "metadata": {
    "id": "yellow-arena"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "import nltk, string, re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Available device : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-november",
   "metadata": {},
   "source": [
    "## Task 1: Preprocessing of Dataset (5 points)\n",
    "\n",
    "\n",
    "\n",
    "1.  Loading and cleaning of Text data:\n",
    "    * Choose 4 categories from the dataset  \n",
    "    * Implement a method `clean_text` which will take text then make text lowercase, remove punctuation, whitespaces and stopwords\n",
    "    * Plot the distribution of classes/categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = None #TODO: Choose 4 categories from the dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_valid = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" Function to perform common NLP pre-processing tasks. \"\"\"\n",
    "    # make lowercase\n",
    "\n",
    "    # remove punctuation\n",
    "\n",
    "    # remove numbers\n",
    "\n",
    "    # remove whitespaces\n",
    "\n",
    "    # remove stopwords\n",
    "\n",
    "    # remove short words\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = []\n",
    "validation_sentences = []\n",
    "\n",
    "train_labels = []\n",
    "validation_labels = []\n",
    "\n",
    "\n",
    "# Clean training sentences\n",
    "for id in range(len(newsgroups_train.data)):\n",
    "    text = clean_text(newsgroups_train.data[id])\n",
    "    label = newsgroups_train.target[id]\n",
    "    if text:\n",
    "        train_sentences.append(text)\n",
    "        train_labels.append(label)\n",
    "\n",
    "# Clean validation sentences\n",
    "for id in range(len(newsgroups_valid.data)):\n",
    "    text = clean_text(newsgroups_valid.data[id])\n",
    "    label = newsgroups_valid.target[id]\n",
    "    if text:\n",
    "        validation_sentences.append(text)\n",
    "        validation_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-anniversary",
   "metadata": {},
   "source": [
    "## Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "# Create vocabulary\n",
    "def build_vocab(sentences, tokenizer):\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(tokenizer(sentence))\n",
    "    return Vocab(counter, specials=['<unk>', '<pad>'])\n",
    "\n",
    "vocabulary = build_vocab(train_sentences, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-filename",
   "metadata": {},
   "source": [
    "## Add padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "\n",
    "# Add Padding \n",
    "def create_dataset(sentences, labels, en_tokenizer, vocab, max_len=128):\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = [vocab[token] for token in en_tokenizer(sentence)]\n",
    "        if len(sentence_tokens) <= max_len:\n",
    "            sentence_tokens = sentence_tokens + [vocab['<pad>']]*(max_len-len(sentence_tokens))\n",
    "        else:\n",
    "            sentence_tokens = sentence_tokens[:max_len]\n",
    "        sentence_tensor = torch.tensor(sentence_tokens,dtype=torch.long)\n",
    "        res.append(sentence_tensor)\n",
    "        \n",
    "    return TensorDataset(torch.stack(res),torch.from_numpy(np.array(labels)))\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PAD_IDX = vocabulary['<pad>']\n",
    "\n",
    "train_dataset = create_dataset(train_sentences,train_labels, en_tokenizer, vocabulary)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "validation_dataset = create_dataset(validation_sentences, validation_labels, en_tokenizer, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-reservoir",
   "metadata": {},
   "source": [
    "## Task 2: Conditional Generative adversarial network definition (5 points)\n",
    "\n",
    "1.  Models Definition:\n",
    "    * Define the Generator & Discriminator network (Achitecture of your choice) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the Generator & Discriminator class\n",
    "class Generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self,output_dim, noise_dim=32):\n",
    "        super(Generator, self).__init__()\n",
    "        pass\n",
    "\n",
    "    # forward method. Condition is should be incorporated to the model input \n",
    "    def forward(self, x):\n",
    "        return None\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self,input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        pass\n",
    "\n",
    "    # forward method. Note: Condition is should be incorporated to the model input\n",
    "    def forward(self, x):\n",
    "        return None\n",
    "\n",
    "# define discriminator and generator\n",
    "# TODO: specify the input and output size\n",
    "\n",
    "D = Discriminator(input_size=None).to(device).float()\n",
    "G = Generator(output_dim=None).to(device).float()\n",
    "\n",
    "print(G)\n",
    "print()\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-delta",
   "metadata": {},
   "source": [
    "## Task 3: Conditional Generative adversarial network training (10 points)\n",
    "\n",
    "* Implement the Conditional Generative adversarial network training procedure \n",
    "* Define the optimizers for Generator and Discriminator network\n",
    "* Define the loss functions\n",
    "* Add Tensorboard to log the Generator and Discriminator loss (for both Training and Validation). For discriminator the loss on fake samples and real samples should be logged separately \n",
    "\n",
    "**NOTE:** It is not important that the loss decreases during the training loop for this task. It is important that the training procedure is correctly implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# params\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 10\n",
    "\n",
    "# TODO: Create optimizers for the discriminator and generator\n",
    "d_optimizer = None\n",
    "g_optimizer = None\n",
    "\n",
    "#fixed noise for validation \n",
    "fixed_noise = torch.normal(0,1, (len(validation_dataset),noise_dim), dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Implement the training procedure and log train & validation loss using tensorboard\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x,y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        #Train discriminator\n",
    "\n",
    "        #Train generator\n",
    "\n",
    "        #validation \n",
    "\n",
    "        #Log Train and validation loss for G and D (training loss for fake and real separate) : Use tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-guest",
   "metadata": {},
   "source": [
    "## Launch Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-sewing",
   "metadata": {},
   "source": [
    "## Task 4: (Optional): Text explainer implemetation using Lime or Shap (5 bonus points)\n",
    "\n",
    "Using the [20 newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) text dataset is used.\n",
    "Create a simple(i.e Decision tree, Random Forest) multi-class classifier and explain the classifiers predictions with the help of LIME or SHARP. \n",
    "\n",
    "**Note:** Use TF-IDF for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "clf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-providence",
   "metadata": {},
   "source": [
    "## <center>Solution should be pushed to github and link to github submitted to Moodle</center>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Midterm Lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
