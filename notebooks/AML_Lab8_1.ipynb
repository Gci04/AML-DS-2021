{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rr4n5-isgvf0"
   },
   "source": [
    "### Week 8: Colaborative Filtering\n",
    "```\n",
    "- Advanced Machine Learning, Innopolis University \n",
    "- Professor: Muhammad Fahim \n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "    1. Content based recommendation Systems \n",
    "    2. Matrix Factorisation\n",
    "    3. Surprise \n",
    "    4. Deep Learning based recommendation systems\n",
    "    5. Lab Task\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WrkNfpCuiZt"
   },
   "source": [
    "## 1. Background\n",
    "\n",
    "**Recommender Systems** are algorithms aimed at suggesting relevant items to users (items being movies to watch, text to read, products to buy or anything else depending on the product).\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1920/1*Y_QG3Kvfk0fSnCirLBHZ7w.jpeg)\n",
    "\n",
    "\n",
    "### Recommendation paradigms\n",
    "\n",
    "The distinction between approaches is more academic than practical, but it’s important to understand their differences.\n",
    "Broadly speaking, recommender systems are of 4 types:\n",
    "\n",
    "1. **Collaborative filtering** is perhaps the most well-known approach to recommendation, to the point that it’s sometimes seen as synonymous with the field. The main idea is that you’re given a matrix of preferences by users for items, and these are used to predict missing preferences and recommend items with high predictions. All you need to get started is user and item IDs and a notion of preference by users for items (ratings, views, etc.).\n",
    "\n",
    "2. **Content-based filtering** algorithms are given user preferences for items and recommend similar items based on a domain-specific notion of item content. This approach also extends naturally to cases where item metadata is available (e.g., movie stars, book authors, and music genres).\n",
    "3. **Social and demographic** recommenders suggest items that are liked by friends, friends of friends, and demographically-similar people. Such recommenders don’t need any preferences by the user to whom recommendations are made, making them very powerful.\n",
    "4. **Contextual recommendation** algorithms recommend items that match the user’s current context. This allows them to be more flexible and adaptive to current user needs than methods that ignore context (essentially giving the same weight to all of the user’s history). Hence, contextual algorithms are more likely to elicit a response than approaches that are based only on historical data.\n",
    "\n",
    "## Collaborative Filtering\n",
    "\n",
    "Collaborative filtering (CF) systems work by collecting user feedback in the form of ratings for items in a given domain and exploiting similarities in rating behavior among several users in determining how to recommend an item.\n",
    "CF accumulates customer product ratings, identifies customers with common ratings, and offers recommendations based on inter-customer comparisons. It’s based on the idea that people who agree in their evaluations of certain items in the past are likely to agree again in the future. For example, most people ask their trusted friends for restaurant or movie suggestions.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/687/1*-Jr1l2rlj9SBcCzlDHtN5g.jpeg)\n",
    "\n",
    "Collaborative filtering models are based on an assumption that people like things similar to other things they like, and things that are liked by other people with similar taste.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1348/1*K5BOY3B93MLn173VVzOW0Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pPUiieErWt8"
   },
   "source": [
    "## 2. Content based recommendation Systems\n",
    "\n",
    "* What is content based recommendation Systems? \n",
    "* How are Content based recommendation Systems different from other systems you know? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRwIQZKwgvf4"
   },
   "source": [
    "### 2.1 Dataset\n",
    "\n",
    "What does the dataset look like? <br>\n",
    "The componets of the dataset:\n",
    "\n",
    "1. Item-ID\n",
    "2. Item-description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JxmZhkBhqtG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "#Load dataset and take a look\n",
    "ds = pd.read_csv(\"sample-data.csv\")\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHriYU-OsLJ9"
   },
   "source": [
    "## 2.2 Recommendation task\n",
    "\n",
    "Recommend k items to a user given that he is intreted in \"Coton Shorts\" item 30.\n",
    "\n",
    "Solution:\n",
    "\n",
    "1. Create TF-IDF of every item.\n",
    "2. Measure cosine distance.\n",
    "3. Propose the K-closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wM1iJ7vzh0fK",
    "outputId": "698ce086-ae93-4541-eb5f-1a1858168843"
   },
   "outputs": [],
   "source": [
    "#Step 1: Create TF-IDF of every item\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(ds['description'])\n",
    "\n",
    "# Step 2: Measure cosine distance\n",
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for idx, row in ds.iterrows():\n",
    "    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n",
    "    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n",
    "\n",
    "    results[row['id']] = similar_items[1:]\n",
    "    \n",
    "print('done!')\n",
    "\n",
    "def item(id):\n",
    "    return ds.loc[ds['id'] == id]['description'].tolist()[0].split(' - ')[0]\n",
    "\n",
    "# Just reads the results out of the dictionary.\n",
    "def recommend(item_id, num):\n",
    "    print(\"Recommending \" + str(num) + \" products similar to \" + item(item_id) + \"...\")\n",
    "    print(\"-------\")\n",
    "    recs = results[item_id][:num]\n",
    "    for rec in recs:\n",
    "        print(\"Recommended: \" + item(rec[1]) + \" (score:\" + str(rec[0]) + \")\")\n",
    "\n",
    "recommend(item_id=30, num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pECAxeDjO0c"
   },
   "source": [
    "Discussion:\n",
    "\n",
    "1. Make changes.\n",
    "2. Evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMVZIHVqq3B5"
   },
   "source": [
    "## 3. Model Based Recommendation Systems\n",
    "\n",
    "\n",
    "![alt text](https://datascienceplus.com/wp-content/uploads/2017/09/2017-09-20-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utdrCOharwcT"
   },
   "source": [
    "## 4.1 Dataset\n",
    "We will use \n",
    "[**MovieLens 20M Dataset**\n",
    "](https://grouplens.org/datasets/movielens/20m/) <br>\n",
    "An open-source dataset available in grouplens.org, The data set has 25000095 ratings and 1093360 tag applications across 62423 movies. Created by 162541 users between 1995 and 2019.\n",
    "\n",
    "Download it and upload the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIsstyT5w8Eg",
    "outputId": "f63c029a-bac1-4118-c273-65f22925d461"
   },
   "outputs": [],
   "source": [
    "!wget https://files.grouplens.org/datasets/movielens/ml-20m.zip --no-check-certificate\n",
    "!unzip 'ml-20m.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAvqcTQDgoIC"
   },
   "source": [
    "## 2.2 Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf24ECSErejk"
   },
   "outputs": [],
   "source": [
    "data_path = './ml-20m/'\n",
    "movies_filename = 'movies.csv'\n",
    "ratings_filename = 'ratings.csv'\n",
    "\n",
    "df_movies = pd.read_csv(\n",
    "    os.path.join(data_path, movies_filename),\n",
    "    usecols=['movieId', 'title'],\n",
    "    dtype={'movieId': 'int32', 'title': 'str'})\n",
    "\n",
    "df_ratings = pd.read_csv(\n",
    "    os.path.join(data_path, ratings_filename),\n",
    "    usecols=['userId', 'movieId', 'rating'],\n",
    "    dtype={'userId': 'int32', 'movieId': 'int32', 'rating': 'float32'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "9jiRYO2cxqro",
    "outputId": "55e6bd28-7f70-4b17-a2a2-dc17f5efbe50"
   },
   "outputs": [],
   "source": [
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "9IQlx3S2x1ae",
    "outputId": "5429ddda-6f12-492f-fb6d-a18e4e2715b8"
   },
   "outputs": [],
   "source": [
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPictqKggz8R"
   },
   "source": [
    "## 2.3 Create user-movie dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMl1Wit2vAI9"
   },
   "outputs": [],
   "source": [
    "df_ratings=df_ratings[:2000000]\n",
    "df_movie_features = df_ratings.pivot(\n",
    "    index='userId',\n",
    "    columns='movieId',\n",
    "    values='rating'\n",
    ").fillna(0)\n",
    "\n",
    "df_movie_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzF2mTfFhJdx"
   },
   "source": [
    "## 2.4 Singular value decomposition (SVD)\n",
    "\n",
    "* Remember dimensionality reduction\n",
    "* What are other algorithmns for dimensionality reduction? \n",
    "\n",
    "SVD from scratch ??? **NO**<br>\n",
    "\n",
    "We will use sklearn implementation for now\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYjtcbQau1Ys"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "R = df_movie_features.values\n",
    "user_ratings_mean = np.mean(R, axis = 1)\n",
    "R_demeaned = R - user_ratings_mean.reshape(-1, 1)\n",
    "\n",
    "U, sigma, Vt = svds(R_demeaned, k = 50)\n",
    "\n",
    "sigma = np.diag(sigma)\n",
    "all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)\n",
    "\n",
    "preds_df = pd.DataFrame(all_user_predicted_ratings, columns = df_movie_features.columns)\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "gRIG32ZyuoI3",
    "outputId": "af036b81-a406-4bc1-9443-aa65aedae732"
   },
   "outputs": [],
   "source": [
    "def recommend_movies(preds_df, userID, movies_df, original_ratings_df, num_recommendations=5):\n",
    "  user_row_number = userID - 1 \n",
    "  sorted_user_predictions = preds_df.iloc[user_row_number].sort_values(ascending=False)\n",
    "  user_data = original_ratings_df[original_ratings_df.userId == (userID)]\n",
    "  user_full = (user_data.merge(movies_df, how = 'left', left_on = 'movieId', right_on = 'movieId').\n",
    "                    sort_values(['rating'], ascending=False)\n",
    "                )\n",
    "  recommendations = (movies_df[~movies_df['movieId'].isin(user_full['movieId'])]).merge(pd.DataFrame(sorted_user_predictions).reset_index(), how = 'left', left_on = 'movieId',\n",
    "              right_on = 'movieId').rename(columns = {user_row_number: 'Predictions'}).sort_values('Predictions', ascending = False).iloc[:num_recommendations, :-1]\n",
    "                    \n",
    "\n",
    "  return user_full, recommendations\n",
    "\n",
    "\n",
    "already_rated, predictions = recommend_movies(preds_df, 330, df_movies, df_ratings, 10)\n",
    "already_rated.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "lwrKNtU9ul_U",
    "outputId": "3b8a70b0-8293-48f0-aece-cb21c805902d"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkgnmVHwgAre"
   },
   "source": [
    "## 3. [Surprise](https://github.com/NicolasHug/Surprise)\n",
    "\n",
    "Surprise is a Python scikit for building and analyzing recommender systems that deal with explicit rating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfIDptgWPfzT"
   },
   "outputs": [],
   "source": [
    "!pip3 install scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10nzFr7qkLUm"
   },
   "source": [
    "### 3.1 Load data and fit SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "722i8WsRQhYg"
   },
   "outputs": [],
   "source": [
    "from surprise import Reader, SVD, Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_ratings[[\"userId\", \"movieId\", \"rating\"]], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99oeMrPXPwFB",
    "outputId": "b092ddfd-645c-45da-886b-b3d6d109e3f0"
   },
   "outputs": [],
   "source": [
    "# Create a train set  and fit the model (using ALS or SGD)\n",
    "trainset = data.build_full_trainset()\n",
    "algo = SVD()\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj_tzfyOlAI4"
   },
   "source": [
    "### 3.2 Recommend Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoHBIdz4QCat"
   },
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "  \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "  Args:\n",
    "      predictions(list of Prediction objects): The list of predictions, as\n",
    "          returned by the test method of an algorithm.\n",
    "      n(int): The number of recommendation to output for each user. Default\n",
    "          is 10.\n",
    "  Returns:\n",
    "  A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "      [(raw item id, rating estimation), ...] of size n.\n",
    "  \"\"\"\n",
    "\n",
    "  # First map the predictions to each user.\n",
    "  top_n = defaultdict(list)\n",
    "  for uid, iid, true_r, est, _ in predictions:\n",
    "      top_n[uid].append((iid, est))\n",
    "\n",
    "  # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "  for uid, user_ratings in top_n.items():\n",
    "      user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "      top_n[uid] = user_ratings[:n]\n",
    "\n",
    "  return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFitDL0UP7CO"
   },
   "outputs": [],
   "source": [
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rg_YiEPscyzO"
   },
   "outputs": [],
   "source": [
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqoNa91Ll1yu"
   },
   "source": [
    "## 4. Deep learning Based approach \n",
    "\n",
    "**We will use PyTorch !!**\n",
    "\n",
    "The deep learnig approach is not so different from SVD what we have just seen earlier. \n",
    "\n",
    "**Back to Embeddings!!!**\n",
    "\n",
    "The Neural network is made up of Two Embedding layers and some hidden layers. \n",
    "\n",
    "1. Example Achitecture :\n",
    "        1.1 Two `Embedding`s for users and movies.\n",
    "        1.2 One `Dropout` for the output of the embeddings.\n",
    "        1.3 The hidden layers\n",
    "        1.4 Output layer\n",
    "\n",
    "2. Example forward pass: \n",
    "        2.1 Get the 2 embeddings tensors, then concatenate both.\n",
    "        2.2 Run it through the hidden layers then the last fc layer.\n",
    "        2.3 Apply sigmoid activation.\n",
    "        2.4 Adjust the range of the estimated rating matrix to be [1, 5].\n",
    "\n",
    "3. Loss function is MSE\n",
    "4. Optimizer ??? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOr7ovka-rcm"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epochs = 100\n",
    "batch_sz = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7svOWRal_MNQ"
   },
   "source": [
    "## Read Data and Create batches\n",
    "\n",
    "100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. It is a smaller dataset for education and development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BC57uXqHmZU1"
   },
   "outputs": [],
   "source": [
    "#Get smaller dataset \n",
    "!wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip --no-check-certificate\n",
    "!unzip ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUCJMTmV34p5"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_ratings = pd.read_csv(\"./ml-latest-small/ratings.csv\", usecols=['userId', 'movieId', 'rating'],\n",
    "                         dtype={'userId': 'int32', 'movieId': 'int32', 'rating': 'float32'})\n",
    "\n",
    "users = df_ratings['userId'].values - 1\n",
    "movies = df_ratings['movieId'].values - 1\n",
    "rates = df_ratings['rating'].values\n",
    "n_samples = len(rates)\n",
    "\n",
    "n_users, n_movies =  max(users)+1, max(movies)+1\n",
    "batches = []\n",
    "\n",
    "#Create batches\n",
    "for i in range(0, n_samples, batch_sz):\n",
    "  limit =  min(i + batch_sz, n_samples)\n",
    "  users_batch, movies_batch, rates_batch = users[i: limit], movies[i: limit], rates[i: limit]\n",
    "  batches.append((torch.tensor(users_batch, dtype=torch.long), torch.tensor(movies_batch, dtype=torch.long),\n",
    "                  torch.tensor(rates_batch, dtype=torch.float)))\n",
    "users = None\n",
    "movies = None \n",
    "rates = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqVdpSR9_P6i"
   },
   "source": [
    "## Define Model\n",
    "\n",
    "**TODO :** implement the hidden layers with the following achitecture: \n",
    "\n",
    "* 3 layers (128, 256 and 128 neurones)\n",
    "* Dropout every after a layer with 20% probability\n",
    "* Relu as activation function for all 3 hidden layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9my9UsmV19yx"
   },
   "outputs": [],
   "source": [
    "class RecommenderNet(nn.Module):\n",
    "  def __init__(self, n_users, n_movies, n_factors=50, embedding_dropout=0.02, dropout_rate=0.2):\n",
    "    super().__init__()\n",
    "\n",
    "    self.u = nn.Embedding(n_users, n_factors)\n",
    "    self.m = nn.Embedding(n_movies, n_factors)\n",
    "    self.drop = nn.Dropout(embedding_dropout)\n",
    "    self.hidden = nn.Sequential(....) #TODO: Implement the hidden layers\n",
    "    self.fc = nn.Linear(128, 1)\n",
    "    self._init()\n",
    "\n",
    "  def forward(self, users, movies, minmax=[1,5]):\n",
    "    features = torch.cat([self.u(users), self.m(movies)], dim=1)\n",
    "    x = self.drop(features)\n",
    "    x = self.hidden(x)\n",
    "    out = torch.sigmoid(self.fc(x))\n",
    "    \n",
    "    if minmax is not None: #Scale the output to [1,5]\n",
    "      min_rating, max_rating = minmax\n",
    "      out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n",
    "    return out\n",
    "\n",
    "  def _init(self):\n",
    "    \"\"\"\n",
    "    Initialize embeddings and hidden layers weights with xavier.\n",
    "    \"\"\"\n",
    "    def init(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "    self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "    self.hidden.apply(init)\n",
    "    init(self.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EnEGN2R3JI2",
    "outputId": "d6409240-b054-4d27-9706-685b7fd96a59"
   },
   "outputs": [],
   "source": [
    "net = RecommenderNet(n_users=n_users, n_movies=n_movies).to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2FkD7_V_WPq"
   },
   "source": [
    "## Define Training parameters\n",
    "\n",
    "1. Loss function \n",
    "2. Optimizer \n",
    "3. learning rate scheduler : dynamic learning rate `lr_scheduler.ReduceLROnPlateau`: Reduce learning rate when a metric has stopped improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_d1gPfz3nuR"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR-4wEv5AFtp"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BixWxZuL3ugQ",
    "outputId": "1df31c61-9cd6-406f-ebd1-77aa92777440"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  train_loss = 0\n",
    "  for users_batch, movies_batch, rates_batch in batches:\n",
    "    net.zero_grad()\n",
    "    out = net(users_batch.to(device), movies_batch.to(device), [1, 5]).squeeze()\n",
    "    loss = criterion(rates_batch.to(device), out)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss\n",
    "  scheduler.step(loss)\n",
    "  print(\"Loss at epoch {} = {}\".format(epoch, loss.item()))\n",
    "print(\"Last Loss = {}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOQxl5o5up-8"
   },
   "source": [
    "## Lab Task\n",
    "```\n",
    "1. Implement ...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "<center>Don't to forget to make a Git commit</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSUY6VFQs5I9"
   },
   "source": [
    "## References\n",
    "1. [Introduction to recommender systems](https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada)\n",
    "\n",
    "2. [Recommender system](https://en.wikipedia.org/wiki/Recommender_system)\n",
    "\n",
    "3. [Recommender Systems with Python — Part I: Content-Based Filtering](https://heartbeat.fritz.ai/recommender-systems-with-python-part-i-content-based-filtering-5df4940bd831)\n",
    "\n",
    "4. [Build a Recommendation Engine With Collaborative Filtering](https://realpython.com/build-recommendation-engine-collaborative-filtering/)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AML_Lab8.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
