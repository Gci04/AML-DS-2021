{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhhBaqkKqjZo"
   },
   "source": [
    "### Week 1: Introduction  \n",
    "```\n",
    "- Advanced Machine Learning, Innopolis University \n",
    "- Professor: Muhammad Fahim \n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "1. Creating a Data Science Repository (i.e Github)\n",
    "2. Deep leaning Frameworks\n",
    "3. About PyTorch and Torchvision installation\n",
    "4. Simple Neural network \n",
    "5. About Tensforflow and tensorboard installation\n",
    "6. Tensorboard example\n",
    "7. Lab Task\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a Data Science Repository\n",
    "\n",
    "Data science code quality is about correctness and reproducibility. Being a Data Scientist doesn't give you permission to write understandable code. \n",
    "\n",
    "### Structure of your code or project layout is very important \n",
    "```Ah thats software engineering stuff ..blah blah blah!```\n",
    "\n",
    "Here are the key points for project sctructure : \n",
    "1. Start with a clean structure\n",
    "2. Make logical structure\n",
    "3. Consistancy\n",
    "4. Information and Knowledge Sharing is key\n",
    "\n",
    "```Remember: You are not working alone!!```\n",
    "\n",
    "### Version Control Systems (Github, GitLab, ... )\n",
    "\n",
    "Porposed simple Structure of project \n",
    "\n",
    "\n",
    "```\n",
    "├── .gitignore               <- .gitignore files in sub folders if needed\n",
    "├── conda_env.yml            <- Conda environment definition \n",
    "├── LICENSE\n",
    "├── README.md                <- The top-level README for developers using this project.\n",
    "├── requirements.txt         <- The requirements file for reproducing the analysis environment, e.g.\n",
    "│                               generated with `pip freeze > requirements.txt`.\n",
    "├── setup.py                 <- Metadata about your project for easy distribution.\n",
    "│\n",
    "├── data                     <- Data Folder\n",
    "├── docs                     <- Documentation\n",
    "│\n",
    "├── notebooks                <- Notebooks for analysis and testing\n",
    "│   ├── eda                  <- Notebooks for EDA\n",
    "│   │   └── example.ipynb    <- Example python notebook\n",
    "│   ├── features             <- Notebooks for generating and analysing features (1 per feature)\n",
    "│   ├── modelling            <- Notebooks for modelling\n",
    "│   └── preprocessing        <- Notebooks for Preprocessing \n",
    "├── scripts                  <- Standalone scripts\n",
    "│   ├── train.py           <- Example sctipt\n",
    "│   ├── test.py              <- Example sctipt\n",
    "│   └── example.py           <- Example sctipt\n",
    "│\n",
    "├── src                      <- Code for use in this project.\n",
    "│\n",
    "└── tests                    <- Test cases\n",
    "    ├── test_notebook.py     <- Example testing that Jupyter notebooks run without errors\n",
    "    └── pipeline             <- pipeline tests\n",
    "\n",
    "```\n",
    "\n",
    "In this course we will use gitHub\n",
    "\n",
    "### ***Git Actions***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO on Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Learning Frameworks \n",
    "\n",
    "The several deep learning frameworks designed & backed by big tech, universities, and researchers.\n",
    "Here is the example of popular : \n",
    "\n",
    "1. Tensorflow\n",
    "1. Keras\n",
    "1. PyTorch\n",
    "1. Apache MXNet\n",
    "1. Microsoft Cognitive Toolkit\n",
    "1. Caffe\n",
    "\n",
    "### In this course we will focus on PyTorch and Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. About PyTorch and Torchvision installation\n",
    "\n",
    "PyTorch a deep learning framework was released by Facebook in 2017 : <br> Installation in python with pip\n",
    "\n",
    "\n",
    "**Main componets :**\n",
    "\n",
    "1. Tensor - Similar to NumPy’s ndarrays, except that tensors can run on accelerators (i.e GPU).\n",
    "2. Datasets & DataLoaders\n",
    "3. Neural Network\n",
    "4. Model Training (back propagation)\n",
    "5. Optimization\n",
    "6. Model Saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch #Installation \n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 [Tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\n",
    "\n",
    "* Can be created from normal list, random or constant values, numpy array and from another tensor\n",
    "* Supports Numpy operations most (tensor.multiply(), ..)\n",
    "* Tensor method with underscore (``_``) performs operations in place (i.e tensor.add_(2))\n",
    "* Can be converted to numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization \n",
    "data = [[5, 6, 1],[3, 4, 5],[4, 3, 6]]\n",
    "tensor_x = torch.tensor(data) # by default in CPU\n",
    "\n",
    "print(f\"Shape of tensor: {tensor_x.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor_x.dtype}\")\n",
    "print(f\"tensor device : {tensor_x.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tensor_x2 = torch.from_numpy(np.ones((5,5)))\n",
    "print(tensor_x2.add(5))\n",
    "print(tensor_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x2.add_(7)\n",
    "print(tensor_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "* For Smooth data loading and preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Neural Network\n",
    "\n",
    "* ```torch.nn``` provides all the building blocks to build own neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # get device for training model \n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,input_dim=50,output_dim=1):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 50)\n",
    "        self.layer2 = nn.Linear(50, 25)\n",
    "        self.output = nn.Linear(25, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.layer1(x)) \n",
    "        x = torch.tanh(self.layer2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "X = torch.rand(1, 50, device=device)\n",
    "print(f\"Neural Network output : {model(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Training\n",
    "\n",
    "* What are the parameters for training a neural network? \n",
    "* What are the steps for training a neural network? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_x = torch.from_numpy(np.random.normal(1,1,(640,50))).float()\n",
    "train_y = torch.from_numpy(np.random.randint(0,2,(640,1))).float()\n",
    "\n",
    "test_x = torch.from_numpy(np.random.normal(1,1,(192,50))).float()\n",
    "test_y = torch.from_numpy(np.random.randint(0,2,(192,1))).float()\n",
    "\n",
    "train_DataLoader = DataLoader(TensorDataset(train_x,train_y), batch_size=64)\n",
    "test_DataLoader = DataLoader(TensorDataset(test_x,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The optimizer and Loss function \n",
    "\n",
    "* ```torch.optim``` module contains different optimizers available in PyTorch such as SGD, ADAM and RMSProp\n",
    "\n",
    "**Inside the training loop**\n",
    "* Reset the gradients of model parameters ```optimizer.zero_grad()```\n",
    "* After foward pass, calculate loss using specified loss function and then backpropagate the loss ```loss.backwards()```\n",
    "* Adjust the weights of the Network ```optimizer.step()```\n",
    "\n",
    "**What Loss function to choose ?**\n",
    "* For Regression\n",
    "* For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Alternatively, you can use a Sequential model\n",
    "# model = nn.Sequential(...).to(device)\n",
    "\n",
    "model = NeuralNetwork().float()\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 100\n",
    "\n",
    "#nn.MSELoss(reduction='mean'), torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    def train_step(x,y):\n",
    "        model.train() #set model to training mode\n",
    "        ypred = model(x) #Foward pass\n",
    "        loss = loss_fn(ypred,y) # Calcutation of loss\n",
    "        loss.backward() \n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    return train_step\n",
    "\n",
    "\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# for epoch in range(n_epochs):\n",
    "#     # Performs one train step and returns the corresponding loss\n",
    "#     for X,y in train_DataLoader:\n",
    "#         loss = train_step(X, y.float())\n",
    "#         losses.append(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to monitor the Neural network training/optimization progress? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Tensforflow, tensorboard installation and example\n",
    "\n",
    "* Tracking and visualizing metrics (Accuracy, loss, ...)\n",
    "* To install tensorboard : ```pip install torch torchvision```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    l = 0.0\n",
    "    size = len(train_DataLoader.dataset)\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    for X,y in train_DataLoader:\n",
    "        loss = train_step(X, y.float())\n",
    "        l += (loss/size)\n",
    "        losses.append(loss)\n",
    "    writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "    for tag, parm in model.named_parameters():\n",
    "        writer.add_histogram(tag, parm.grad.data.cpu().numpy(), epoch)\n",
    "\n",
    "dummy_input = torch.autograd.Variable(torch.rand(1, 50))\n",
    "writer.add_graph(model=model, input_to_model=(dummy_input, ))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to save model for later use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about Tensorflow or Keras\n",
    "\n",
    "* Tensorflow 2.0 stable version was released in October 2019 \n",
    "* Tensorflow 2.0 looks more like Keras\n",
    "* Represent computation as a directed acyclic graph often called Computation Graph -- same as PyTorch\n",
    "* Tensorflow has static computation graph while PyTorch has Dynamic computational graph (One of major differences)\n",
    "* **Now its difficult to separate the two since they both support static and dynamic**\n",
    "\n",
    "There are 3 ways (That I know) to create a and train neural network in tensorflow 2.0 <br>\n",
    "Let's see how a neural net looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "## Data\n",
    "SIZE = 2000\n",
    "EPOCHS = 10\n",
    "x1 = np.random.normal(0, 10, SIZE)\n",
    "x2 = np.random.normal(3, 5, SIZE)\n",
    "x = np.dstack((x1, x2))[0]\n",
    "y = (3 * np.sqrt(np.abs(x1)+0.1) + np.power(x2,3)).reshape(-1,1)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(100).batch(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential()\n",
    "model1.add(tf.keras.layers.Dense(64, input_shape=(2,) , activation='tanh'))\n",
    "model1.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
    "model1.add(tf.keras.layers.Dense(1))\n",
    "model1.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=tf.keras.losses.MSE)\n",
    "History = model1.fit(train_dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 \n",
    "\n",
    "* tf.keras.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.hidden1 = tf.keras.layers.Dense(64, input_shape=(2,) ,activation='sigmoid')\n",
    "        self.hidden2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(2)\n",
    "        self.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=tf.keras.losses.MSE)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.hidden1(inputs)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "model2 = Model2()\n",
    "model2.fit(train_dataset, epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3\n",
    "\n",
    "* `tf.GradientTape()` (Mostly used in training of GANs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.Sequential()\n",
    "model3.add(tf.keras.layers.Dense(64, input_shape=(2,) , activation='sigmoid'))\n",
    "model3.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
    "model3.add(tf.keras.layers.Dense(1))\n",
    "# model3.build()\n",
    "opt =tf.keras.optimizers.Adam(0.001)\n",
    "mse =tf.keras.losses.MSE\n",
    "batch = 100\n",
    "for i in range(10):\n",
    "    for X_sample, y_sample in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pre = model3(X_sample)\n",
    "            loss = mse(y_sample, y_pre)\n",
    "        grads = tape.gradient(loss, model3.trainable_variables)\t\n",
    "        processed_grads = [g for g in grads]\n",
    "        grads_and_vars = zip(processed_grads, model3.trainable_variables)\n",
    "        opt.apply_gradients(grads_and_vars)\n",
    "    print ('Model3: ', i, ' : ', np.mean(loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the model \n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "\n",
    "model11 = tf.keras.Sequential()\n",
    "model11.add(tf.keras.layers.Dense(64, input_shape=(2,) , activation='tanh'))\n",
    "model11.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
    "model11.add(tf.keras.layers.Dense(1))\n",
    "model11.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=tf.keras.losses.MSE)\n",
    "\n",
    "\n",
    "x1_val = np.random.normal(0, 10, (30,1))\n",
    "x2_val = np.random.normal(3, 5, (30,1))\n",
    "x_val = np.hstack((x1_val, x2_val))\n",
    "y_val = (3 * np.sqrt(np.abs(x1_val)+0.1) + np.power(x2_val,3))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "model11.fit(train_dataset, epochs=5, validation_data=(x_val,y_val), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tensorboard example](https://colab.research.google.com/drive/1_HxMqsqEAHAo2vZ9aER7jCbtOlJJ4fOf?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab task \n",
    "\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "1. Create a GitHub or GitLab private repo (add Gci04 as contributor)\n",
    "2. Use the example given in lab and modify it to regression task\n",
    "3. Create tests (minimum 2 in tests folder)\n",
    "4. Use Tensorboard\n",
    "5. Modify the test script for your model and git actions script if necessary\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AML - Lab2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
