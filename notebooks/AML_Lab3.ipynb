{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SMM2RQWYh-0"
   },
   "source": [
    "### Reccurent Neural Networks\n",
    "```\n",
    "- Advanced Machine Learning, Innopolis University \n",
    "- Professor: Muhammad Fahim \n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "1. Movie Sentiment Analysis\n",
    "    a. Dataset\n",
    "    b. Data Preprocessing\n",
    "    c. PyTorch RNN \n",
    "    d. Keras Simple Neural Network \n",
    "    e. Keras Convolutional Neural Network\n",
    "    f. Lab Task\n",
    "```\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3edS-8CFYcAe"
   },
   "source": [
    "## Dataset Description\n",
    "\n",
    "[IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/) having 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX8Y56qHYcAl"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "[`torchtext`](https://pytorch.org/text/stable/index.html) is a package that consists of data processing utilities and popular datasets for natural language\n",
    "\n",
    "One of the main concepts of TorchText is the `Field`. To define how the data should be processed we will use `Field`. Our input data contains raw strings <br>\n",
    "The declared `TEXT` field defines how the review should be processed, and the `LABEL` field to process the sentiment. \n",
    "\n",
    "For more on `Fields`, go [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eFGKi6vXYcAn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                  tokenizer_language = 'en_core_web_sm')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_Dg1WbbYcAn"
   },
   "source": [
    "## Download the data\n",
    "The following code automatically downloads the IMDb dataset and splits it into the canonical train/test splits as `torchtext.datasets` objects. It process the data using the `Fields` we have previously defined. The IMDb dataset consists of 50,000 movie reviews, each marked as being a positive or negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhMqmCDkYcAo",
    "outputId": "0f6acfd7-0ce2-4092-f5d2-d27c2b09ee7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "aclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 62.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 training examples\n",
      "25000 testing examples\n"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "print(f'{len(train_data)} training examples')\n",
    "print(f'{len(test_data)} testing examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkUWxRVvYcAr"
   },
   "source": [
    "## Split the data to train and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XnkIgmfwYcAr"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfwt62cLYcAr"
   },
   "source": [
    "Again, we'll view how many examples are in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33-NKWj7YcAs",
    "outputId": "8ac80eca-6358-478c-a13e-53765d136324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4CGxi9kIYcAt"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc3K35fQYcAu"
   },
   "source": [
    "Why do we only build the vocabulary on the training set? When testing any machine learning system you do not want to look at the test set in any way. We do not include the validation set as we want it to reflect the test set as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiirlj6fYcAu",
    "outputId": "a61ee396-25a2-413d-8eb1-32855697cfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgjGe4DhYcAx"
   },
   "source": [
    "## Dataloders / Iterators \n",
    "\n",
    "* We have done preprocessing of the raw data but we have to create batches and convert the data to tensors. For text data Pytorch provides a container called `BucketIterator` for such task.\n",
    "\n",
    "* The `BucketIterator` will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
    "\n",
    "* To put the data into the training device, its neccesary to specify the device parameter in the `BucketIterator` then Pytorch will take care of the rest. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MZarz-5NYcAx"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data), \n",
    "    batch_size = batch_size,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxH-cBsQYcAx"
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "The next stage is building the model that we'll eventually train and evaluate. \n",
    "\n",
    "There is a small amount of boilerplate code when creating models in PyTorch, note how our `RNN` class is a sub-class of `nn.Module` and the use of `super`.\n",
    "\n",
    "Within the `__init__` we define the _layers_ of the module. Our three layers are an _embedding_ layer, our RNN, and a _linear_ layer. All layers have their parameters initialized to random values, unless explicitly specified.\n",
    "\n",
    "\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment7.png?raw=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LKf7tztuYcAz"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.embedding_layer = nn.Embedding(input_dim, embedding_dim)\n",
    "    self.rnn_cell = nn.RNN(embedding_dim, hidden_dim)\n",
    "    self.fc_layer = nn.Linear(hidden_dim, output_dim)\n",
    "      \n",
    "  def forward(self, text):\n",
    "    \"\"\"\n",
    "    Foward pass\n",
    "    Args:\n",
    "        text:\n",
    "            sentiment text with shape [sentence length, batch size]\n",
    "    \"\"\"\n",
    "    embedded = self.embedding_layer(text) # embedding_layer output shape  (sentence length, batch size, embedding dim]\n",
    "    output, hidden = self.rnn_cell(embedded) \n",
    "    \n",
    "    assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "    \n",
    "    return self.fc_layer(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "x5G_zCcUYcA0"
   },
   "outputs": [],
   "source": [
    "input_dim = len(TEXT.vocab) #input dimension is the dimension of the one-hot vectors\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256 #size of the hidden states\n",
    "output_dim = 1 # for the fully connected \n",
    "\n",
    "model = RNN(input_dim, embedding_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8mzk7joYcA1"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OSjst1pIYcA2"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# define loss function and optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#make model instance and send it to training device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1Gfwn1NiYcA3"
   },
   "outputs": [],
   "source": [
    "# accuracy_calculator which takes predicted labels and real labels\n",
    "def accuracy_calculator(preds, y):\n",
    "  \"\"\"Returns accuracy per batch\"\"\"\n",
    "  rounded_preds = torch.round(torch.sigmoid(preds)) #round predictions to the closest integer\n",
    "  correct = (rounded_preds == y).float()\n",
    "  return correct.sum() / len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DVcX-RZiYcA4"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "  \n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  \n",
    "  model.train()\n",
    "  \n",
    "  for batch in iterator:\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(batch.text).squeeze(1)\n",
    "    loss = criterion(predictions, batch.label)\n",
    "\n",
    "    acc = accuracy_calculator(predictions, batch.label)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epoch_loss += loss.item()\n",
    "    epoch_acc += acc.item()\n",
    "      \n",
    "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nJQQ4_lxYcA4"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_batches, criterion):\n",
    "  eval_loss = 0\n",
    "  eval_acc = 0\n",
    "  \n",
    "  model.eval()\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for batch in data_batches:\n",
    "      predictions = model(batch.text).squeeze(1)\n",
    "      loss = criterion(predictions, batch.label)\n",
    "      \n",
    "      acc = accuracy_calculator(predictions, batch.label)\n",
    "      eval_loss += loss.item()\n",
    "      eval_acc += acc.item()\n",
    "  \n",
    "  return eval_loss / len(data_batches), eval_acc / len(data_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwqbAPboYcA5",
    "outputId": "37300ca3-8fa5-423b-8b08-a18de04fddb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Train [Loss:  0.694  Acc :50.18], Val.[Loss: 0.696 Acc: 50.87]\n",
      "Epoch: 2 , Train [Loss:  0.693  Acc :49.62], Val.[Loss: 0.696 Acc: 49.71]\n",
      "Epoch: 4 , Train [Loss:  0.693  Acc :49.81], Val.[Loss: 0.696 Acc: 49.54]\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "  valid_loss, valid_acc = evaluate_model(model, valid_iterator, criterion)\n",
    "  \n",
    "  if valid_loss < best_valid_loss:\n",
    "    best_valid_loss = valid_loss\n",
    "    torch.save(model.state_dict(), 'best-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} , Train [Loss:  {train_loss:.3f}  Acc :{train_acc*100:.2f}], Val.[Loss: {valid_loss:.3f} Acc: {valid_acc*100:.2f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPqbylW4zRCz"
   },
   "source": [
    "## Save and load model for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1lECvJOYcA6",
    "outputId": "c75754f4-cfb8-4d09-92d6-21e048936622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data : 47.64%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-model.pt')) #Load the best model\n",
    "test_loss, test_acc = evaluate_model(model, test_iterator, criterion)\n",
    "print(f'Accuracy on test data : {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMeZxj-wCwqg"
   },
   "source": [
    "## More on Tensorflow and Keras\n",
    "```\n",
    "1. Data Preprocessing\n",
    "2. Simple Neural Network\n",
    "3. Convolutional Neural Network\n",
    "4. Recurrent Neural Network \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWqBtEhnDIyF"
   },
   "source": [
    "## Download, unzip and read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WcXyrYUAFnf"
   },
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "\n",
    "import pandas as pd\n",
    "import wget, zipfile\n",
    "\n",
    "wget.download('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv')\n",
    "\n",
    "movie_reviews = pd.read_csv(\"IMDb_Reviews.csv\")\n",
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "5Qn2raI4BZhM",
    "outputId": "fa07421d-66f1-4cb9-df46-bd8dec7853d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff9365c6750>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize class distribution (balanced or not).\n",
    "sns.countplot(x='sentiment', data=movie_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Jxwk0qtECL6"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lcT1l3dZEWyR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, Conv1D, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lfSiEcT1EAMh"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "  # Removing tags\n",
    "  sentence = remove_tags(sen)\n",
    "\n",
    "  # Remove punctuations and numbers\n",
    "  sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "  # Single character removal\n",
    "  sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "  # Removing multiple spaces\n",
    "  sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "  return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "P-eSbNgPENDT"
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "X = []\n",
    "sentences = list(movie_reviews['review'])\n",
    "for sen in sentences:\n",
    "  X.append(preprocess_text(sen))\n",
    "\n",
    "#Split the data to train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, movie_reviews['sentiment'].values, test_size=0.20, random_state=42)\n",
    "\n",
    "#Tokenize the data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PozGpmBQE7W7"
   },
   "outputs": [],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKRG1QCyFFdo"
   },
   "source": [
    "### Get pre-trained embeddings (Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tJwySSAyIxw0"
   },
   "outputs": [],
   "source": [
    "# Download Glove embeddings from http://nlp.stanford.edu/data/glove.6B.zip and unzip.\n",
    "wget.download(\"http://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "with zipfile.ZipFile(\"glove.6B.zip\", 'r') as zip_ref:\n",
    "  zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "fbvBbVKXFMMf"
   },
   "outputs": [],
   "source": [
    "# Retrieve the embeddings and embedding matrix \n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in glove_file:\n",
    "  records = line.split()\n",
    "  word = records[0]\n",
    "  vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "  embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "  embedding_vector = embeddings_dictionary.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEELt92qFf1d"
   },
   "source": [
    "### Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UM4GbiQ3Fkbt",
    "outputId": "ac860296-45f6-49c5-923e-556795c2055c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          9230300   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 9,872,717\n",
      "Trainable params: 642,417\n",
      "Non-trainable params: 9,230,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Create, add Embedding layer and freeze it \n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "#Flatten the embedding output \n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(8, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Specify the optimizer, loss function and metric \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rdrmKl-F50Z",
    "outputId": "d02f5dd8-5179-4854-945d-07dd44aefad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.0749 - acc: 0.7098\n",
      "Test Score: 1.0749452114105225\n",
      "Test Accuracy: 0.7098000049591064\n"
     ]
    }
   ],
   "source": [
    "# Train the model by calling the fit method. Specify the batch size, number of epochs and validation (optional)\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=0, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-E0FPWiLF_5I"
   },
   "outputs": [],
   "source": [
    "#TODO: Visualize the accuracy and loss of the model using i.e matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02ktYL-UGRtw"
   },
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcs3L2c2GRIi",
    "outputId": "d793b8a5-e83b-41e7-8eee-5587b50df297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          9230300   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 96, 128)           64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 9,294,557\n",
      "Trainable params: 64,257\n",
      "Non-trainable params: 9,230,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgsjjHTYG_wv",
    "outputId": "06bc76ee-22af-4e76-a8f2-727f9b4b929f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3482 - acc: 0.8491\n",
      "Test Score: 0.3481959104537964\n",
      "Test Accuracy: 0.8490999937057495\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=0, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "uOstqtek5kaq"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text, tokenizer,maxlen):\n",
    "  processed_text = [preprocess_text(text)]\n",
    "  text_sequences = tokenizer.texts_to_sequences(processed_text)\n",
    "  padded_text = pad_sequences(text_sequences, padding='post', maxlen=maxlen)\n",
    "  \n",
    "  pred_sent = model.predict(padded_text)\n",
    "  \n",
    "  if (np.round(pred_sent) == 1.0):\n",
    "    print(\"Positive sentiment\")\n",
    "  else:\n",
    "    print(\"Negative sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKJ6eu9t6bYx",
    "outputId": "883358e4-f012-4c20-f972-e0cf4cdb155a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(model,\"it was movie i have watched in my the last ten years\", tokenizer, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uK0GklzYcA6"
   },
   "source": [
    "## Lab Task\n",
    "```\n",
    "1. Make improvements of the current RNN by : \n",
    "  - Usin pre-trained word embeddings (GloVe)\n",
    "  - different RNN architecture\n",
    "  - multi-layer RNN\n",
    "  - a different optimizer\n",
    "2. Calculate number of model parameters (traninable and non-trainable)\n",
    "3. Implement a method that will take a string (review sentence) and return a sentiment of that string\n",
    "\n",
    "**Try to get accuracy > 80%\n",
    "```\n",
    "\n",
    "<b>Note: </b> [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) <br>\n",
    "<center>Don't to forget to make a Git commit</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLNTZebOI45W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AML_Lab3_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
