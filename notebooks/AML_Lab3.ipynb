{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AML_Lab3_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SMM2RQWYh-0"
      },
      "source": [
        "### Week 3: Reccurent Neural Networks\n",
        "```\n",
        "- Advanced Machine Learning, Innopolis University \n",
        "- Professor: Muhammad Fahim \n",
        "- Teaching Assistant: Gcinizwe Dlamini\n",
        "```\n",
        "<hr>\n",
        "\n",
        "\n",
        "```\n",
        "Lab Plan\n",
        "1. Movie Sentiment Analysis\n",
        "    a. Dataset\n",
        "    b. Data Preprocessing\n",
        "    c. PyTorch RNN \n",
        "    d. Keras Simple Neural Network \n",
        "    e. Keras Convolutional Neural Network\n",
        "    f. Lab Task\n",
        "```\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3edS-8CFYcAe"
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "[IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/) having 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX8Y56qHYcAl"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "[`torchtext`](https://pytorch.org/text/stable/index.html) is a package that consists of data processing utilities and popular datasets for natural language\n",
        "\n",
        "One of the main concepts of TorchText is the `Field`. To define how the data should be processed we will use `Field`. Our input data contains raw strings <br>\n",
        "The declared `TEXT` field defines how the review should be processed, and the `LABEL` field to process the sentiment. \n",
        "\n",
        "For more on `Fields`, go [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFGKi6vXYcAn"
      },
      "source": [
        "import torch\n",
        "from torchtext.legacy import data\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm')\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Dg1WbbYcAn"
      },
      "source": [
        "## Download the data\n",
        "The following code automatically downloads the IMDb dataset and splits it into the canonical train/test splits as `torchtext.datasets` objects. It process the data using the `Fields` we have previously defined. The IMDb dataset consists of 50,000 movie reviews, each marked as being a positive or negative review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhMqmCDkYcAo",
        "outputId": "2bc4e961-1c8e-4d0f-d38e-85f8bf59dd80"
      },
      "source": [
        "from torchtext.legacy import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "print(f'{len(train_data)} training examples')\n",
        "print(f'{len(test_data)} testing examples')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 11.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25000 training examples\n",
            "25000 testing examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w42fi2uaYcAq",
        "outputId": "ac8fa698-b294-4bba-9d3e-1626e2be4592"
      },
      "source": [
        "print(vars(train_data.examples[5]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['Despite', 'gorgeous', 'and', 'breathtaking', 'animation', ',', 'this', 'is', 'probably', 'one', 'of', 'most', 'uninspiring', 'Disney', 'films', 'I', \"'ve\", 'seen', ',', 'and', 'I', 'do', \"n't\", 'slam', 'Disney', 'films', 'very', 'often', '.', 'Spirit', 'is', 'a', 'wild', 'stallion', 'who', 'repeatedly', 'gets', 'captured', ',', 'either', 'by', 'the', 'cavalry', 'or', 'by', 'Indians', ',', 'both', 'of', 'which', 'try', 'to', '\"', 'break', '\"', 'him', '.', 'Spirit', 'ends', 'up', 'forming', 'a', 'bond', 'with', 'the', 'Indian', ',', 'and', 'that', ',', 'in', 'a', 'nutshell', ',', 'is', 'the', 'story', '.', 'With', 'exception', 'to', 'the', 'beautiful', 'animation', 'of', 'the', 'horses', ',', 'neither', 'I', 'or', 'my', 'five', 'year', 'old', 'were', 'very', 'inspired', 'or', 'excited', 'by', 'this', 'film', '.', 'It', \"'s\", 'ironic', 'that', 'it', \"'s\", 'titled', '\"', 'Spirit', '\"', ',', 'as', 'spirit', 'is', 'what', 'this', 'film', 'could', 'have', 'used', 'a', 'bit', 'more', 'of', '.', 'An', 'extra', 'point', 'was', 'given', 'for', 'the', 'soundtrack', ',', 'which', 'was', 'enjoyable', ',', 'with', 'songs', 'by', 'Bryan', 'Adams', 'and', 'Hans', 'Zimmer', '.', 'And', 'although', 'this', 'film', 'is', 'rated', 'G', ',', 'you', 'will', 'still', 'probably', 'have', 'to', 'end', 'up', 'explaining', 'what', '\"', 'breaking', 'a', 'horse', '\"', 'means', 'to', 'your', 'five', 'year', 'old', '.', 'I', 'did.<br', '/><br', '/', '>'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkUWxRVvYcAr"
      },
      "source": [
        "## Split the data to train and validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnkIgmfwYcAr"
      },
      "source": [
        "import random\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfwt62cLYcAr"
      },
      "source": [
        "Again, we'll view how many examples are in each split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33-NKWj7YcAs",
        "outputId": "32836adc-f647-4fa5-bc88-cfb810d1ebb8"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 17500\n",
            "Number of validation examples: 7500\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CGxi9kIYcAt"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "import torchtext.vocab as vocab\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "# TEXT.build_vocab(train_data, vectors='glove.6B.100d')\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc3K35fQYcAu"
      },
      "source": [
        "Why do we only build the vocabulary on the training set? When testing any machine learning system you do not want to look at the test set in any way. We do not include the validation set as we want it to reflect the test set as much as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiirlj6fYcAu",
        "outputId": "e8071e3a-1b00-45ba-97d9-104a3d56662d"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0EGn7cwYcAw"
      },
      "source": [
        "We can also check the labels, 0 is for negative and 1 is for positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoB8WOCSYcAw",
        "outputId": "166e680e-68a0-43f3-a610-d51c4d8c1cc2"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(None, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgjGe4DhYcAx"
      },
      "source": [
        "## Dataloders / Iterators \n",
        "\n",
        "* We have done preprocessing of the raw data but we have to create batches and convert the data to tensors. For text data Pytorch provides a container called `BucketIterator` for such task.\n",
        "\n",
        "* The `BucketIterator` will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
        "\n",
        "* To put the data into the training device, its neccesary to specify the device parameter in the `BucketIterator` then Pytorch will take care of the rest. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZarz-5NYcAx"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data), \n",
        "    batch_size = batch_size,\n",
        "    device = device)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxH-cBsQYcAx"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "The next stage is building the model that we'll eventually train and evaluate. \n",
        "\n",
        "There is a small amount of boilerplate code when creating models in PyTorch, note how our `RNN` class is a sub-class of `nn.Module` and the use of `super`.\n",
        "\n",
        "Within the `__init__` we define the _layers_ of the module. Our three layers are an _embedding_ layer, our RNN, and a _linear_ layer. All layers have their parameters initialized to random values, unless explicitly specified.\n",
        "\n",
        "\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment7.png?raw=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKf7tztuYcAz"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.embedding_layer = nn.Embedding(input_dim, embedding_dim)\n",
        "    self.rnn_cell = nn.RNN(embedding_dim, hidden_dim)\n",
        "    self.fc_layer = nn.Linear(hidden_dim, output_dim)\n",
        "      \n",
        "  def forward(self, text):\n",
        "    \"\"\"\n",
        "    Foward pass\n",
        "    Args:\n",
        "        text:\n",
        "            sentiment text with shape [sentence length, batch size]\n",
        "    \"\"\"\n",
        "    embedded = self.embedding_layer(text) # embedding_layer output shape  (sentence length, batch size, embedding dim]\n",
        "    output, hidden = self.rnn_cell(embedded) # \n",
        "    \n",
        "    assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "    \n",
        "    return self.fc_layer(hidden.squeeze(0))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5G_zCcUYcA0"
      },
      "source": [
        "input_dim = len(TEXT.vocab) #input dimension is the dimension of the one-hot vectors\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256 #size of the hidden states\n",
        "output_dim = 1 # for the fully connected \n",
        "\n",
        "model = RNN(input_dim, embedding_dim, hidden_dim, output_dim)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8mzk7joYcA1"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSjst1pIYcA2"
      },
      "source": [
        "import torch.optim as optim\n",
        "# define loss function and optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#make model instance and send it to training device\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gfwn1NiYcA3"
      },
      "source": [
        "# TODO: Implement accuracy_calculator which takes predicted labels and real labels\n",
        "def accuracy_calculator(preds, y):\n",
        "  \"\"\"Returns accuracy per batch\"\"\"\n",
        "  return None"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVcX-RZiYcA4"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "  \n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  \n",
        "  model.train()\n",
        "  \n",
        "  for batch in iterator:\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(batch.text).squeeze(1)\n",
        "    loss = criterion(predictions, batch.label)\n",
        "\n",
        "    acc = accuracy_calculator(predictions, batch.label)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "      \n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJQQ4_lxYcA4"
      },
      "source": [
        "def evaluate_model(model, data_batches, criterion):\n",
        "  eval_loss = 0\n",
        "  eval_acc = 0\n",
        "  \n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for batch in data_batches:\n",
        "      predictions = model(batch.text).squeeze(1)\n",
        "      loss = criterion(predictions, batch.label)\n",
        "      \n",
        "      acc = accuracy_calculator(predictions, batch.label)\n",
        "      eval_loss += loss.item()\n",
        "      eval_acc += acc.item()\n",
        "  \n",
        "  return eval_loss / len(data_batches), eval_acc / len(data_batches)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwqbAPboYcA5",
        "outputId": "3a50bfb4-d85a-4cc3-be69-3e9888a0bab1"
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate_model(model, valid_iterator, criterion)\n",
        "  \n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), 'best-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1} , Train [Loss:  {train_loss:.3f}  Acc :{train_acc*100:.2f}], Val.[Loss: {valid_loss:.3f} Acc: {valid_acc*100:.2f}]')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 , Train [Loss:  0.694  Acc :50.17], Val.[Loss: 0.698 Acc: 49.73]\n",
            "Epoch: 2 , Train [Loss:  0.693  Acc :49.54], Val.[Loss: 0.698 Acc: 49.24]\n",
            "Epoch: 4 , Train [Loss:  0.693  Acc :49.53], Val.[Loss: 0.698 Acc: 48.95]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPqbylW4zRCz"
      },
      "source": [
        "## Save and load model for Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1lECvJOYcA6",
        "outputId": "156288fe-b244-4173-fa92-b33d65fb16a4"
      },
      "source": [
        "model.load_state_dict(torch.load('best-model.pt')) #Load the best model\n",
        "test_loss, test_acc = evaluate_model(model, test_iterator, criterion)\n",
        "print(f'Accuracy on test data : {test_acc*100:.2f}%')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test data : 47.90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMeZxj-wCwqg"
      },
      "source": [
        "## More on Tensorflow and Keras\n",
        "```\n",
        "1. Data Preprocessing\n",
        "2. Simple Neural Network\n",
        "3. Convolutional Neural Network\n",
        "4. Recurrent Neural Network \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWqBtEhnDIyF"
      },
      "source": [
        "## Download, unzip and read dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "6WcXyrYUAFnf",
        "outputId": "5924aeb6-2169-4dd7-f55e-9d823f8ed956"
      },
      "source": [
        "!pip install wget\n",
        "\n",
        "import pandas as pd\n",
        "import wget, zipfile\n",
        "\n",
        "wget.download('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv')\n",
        "# !wget https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\n",
        "\n",
        "movie_reviews = pd.read_csv(\"IMDb_Reviews.csv\")\n",
        "movie_reviews.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=82fdecd86e590139a590372c7a086dae5d3b599fba2bc4eb1b04a57ed6a821e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>My family and I normally do not watch local mo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Believe it or not, this was at one time the wo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>After some internet surfing, I found the \"Home...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>One of the most unheralded great works of anim...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  My family and I normally do not watch local mo...          1\n",
              "1  Believe it or not, this was at one time the wo...          0\n",
              "2  After some internet surfing, I found the \"Home...          0\n",
              "3  One of the most unheralded great works of anim...          1\n",
              "4  It was the Sixties, and anyone with long hair ...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Qn2raI4BZhM",
        "outputId": "a273e205-e1ca-43e7-8e02-da573d4f83dd"
      },
      "source": [
        "# TODO: See Visualize class distribution (balanced or not). i.e use matplotlib or plotly\n",
        "print(movie_reviews[\"review\"][3])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One of the most unheralded great works of animation. Though it makes the most sophisticated use of the \"cut-out\" method of animation (a la \"South Park\"), the real talent behind \"Twice Upon a Time\" are the vocal characterizations, with Lorenzo Music's (Carlton from TV's \"Rhoda\") Woody Allen-ish Ralph-the-all-purpose-Animal being the centerpiece. The \"accidental nightmare\" sequence is doubtless one of the best pieces of animation ever filmed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jxwk0qtECL6"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcT1l3dZEWyR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, Conv1D, LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfSiEcT1EAMh"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "  # Removing tags\n",
        "  sentence = remove_tags(sen)\n",
        "\n",
        "  # Remove punctuations and numbers\n",
        "  sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "  # Single character removal\n",
        "  sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "  # Removing multiple spaces\n",
        "  sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "  return sentence\n",
        "\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "  return TAG_RE.sub('', text)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-eSbNgPENDT"
      },
      "source": [
        "# Clean the data\n",
        "X = []\n",
        "sentences = list(movie_reviews['review'])\n",
        "for sen in sentences:\n",
        "  X.append(preprocess_text(sen))\n",
        "\n",
        "#Split the data to train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, movie_reviews['sentiment'].values, test_size=0.20, random_state=42)\n",
        "\n",
        "#Tokenize the data\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PozGpmBQE7W7"
      },
      "source": [
        "# Adding 1 because of reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKRG1QCyFFdo"
      },
      "source": [
        "### Get pre-trained embeddings (Glove)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJwySSAyIxw0"
      },
      "source": [
        "# TODO : Download Glove embeddings from http://nlp.stanford.edu/data/glove.6B.zip and unzip.\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbvBbVKXFMMf"
      },
      "source": [
        "# Retrieve the embeddings and embedding matrix \n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "for line in glove_file:\n",
        "  records = line.split()\n",
        "  word = records[0]\n",
        "  vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
        "  embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  embedding_vector = embeddings_dictionary.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEELt92qFf1d"
      },
      "source": [
        "### Simple Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM4GbiQ3Fkbt",
        "outputId": "13a2102e-e05b-4522-9cde-744f74a2b53d"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Create, add Embedding layer and freeze it \n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "model.add(embedding_layer)\n",
        "\n",
        "#Flatten the embedding output \n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='sigmoid'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(8, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#Specify the optimizer, loss function and metric \n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          9230300   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                640064    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 264       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 9,872,717\n",
            "Trainable params: 642,417\n",
            "Non-trainable params: 9,230,300\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rdrmKl-F50Z",
        "outputId": "59b14e3b-a46e-4745-a204-bef6d3ea0ef6"
      },
      "source": [
        "# Train the model by calling the fit method. Specify the batch size, number of epochs and validation (optional)\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=0, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on test set\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6476 - acc: 0.7277\n",
            "Test Score: 0.6476041078567505\n",
            "Test Accuracy: 0.7276999950408936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E0FPWiLF_5I"
      },
      "source": [
        "#TODO: Visualize the accuracy and loss of the model using i.e matplotlib.pyplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02ktYL-UGRtw"
      },
      "source": [
        "### Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcs3L2c2GRIi",
        "outputId": "21834777-097e-4df2-e907-85f439bffb76"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "model.add(embedding_layer)\n",
        "\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 100)          9230300   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 96, 128)           64128     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 9,294,557\n",
            "Trainable params: 64,257\n",
            "Non-trainable params: 9,230,300\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgsjjHTYG_wv",
        "outputId": "2ec0895d-fa48-4a39-dd36-290e69119ba7"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=0, validation_split=0.2)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8333\n",
            "Test Score: 0.3914513885974884\n",
            "Test Accuracy: 0.833299994468689\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPPrL-YNHlMb"
      },
      "source": [
        "### Recurrent Neural Network (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inLKlHMHHD56"
      },
      "source": [
        "#TODO(optional): Recurrent Neural Network (LSTM) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uK0GklzYcA6"
      },
      "source": [
        "## Lab Task\n",
        "```\n",
        "1. Make improvements of the current RNN by : \n",
        "  - Usin pre-trained word embeddings (GloVe)\n",
        "  - different RNN architecture\n",
        "  - multi-layer RNN or LSTM\n",
        "  - a different optimizer\n",
        "2. Calculate number of model parameters (traninable and non-trainable)\n",
        "3. Implement a method that will take a string (review sentence) and return a sentiment of that string\n",
        "\n",
        "**Try to get accuracy > 80%\n",
        "```\n",
        "\n",
        "<b>Note: </b> [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) <br>\n",
        "<center>Don't to forget to make a Git commit</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLNTZebOI45W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}