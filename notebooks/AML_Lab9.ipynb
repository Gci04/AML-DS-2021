{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 9 : Generative Adversarial Networks\n",
    "```\n",
    "- Advanced Machine Learning, Innopolis University \n",
    "- Professor: Muhammad Fahim \n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "    1. Homework 1 Feedback\n",
    "    2. Recap Auto-encoder\n",
    "    3. Vanila GAN\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Autoencoders (Recap)\n",
    "\n",
    "* Types of autoencoders\n",
    "* Applications of autoencoders\n",
    "* Autoencoders training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![caption](https://miro.medium.com/max/2400/1*Q5dogodt3wzKKktE0v3dMQ@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Simple autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim):\n",
    "        # Step 1 : Define the encoder \n",
    "        # Step 2 : Define the decoder\n",
    "        # Step 3 : Initialize the weights (optional)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Step 1: Pass the input through encoder to get latent representation\n",
    "        # Step 2: Take latent representation and pass through decoder\n",
    "        pass\n",
    "    \n",
    "    def encode(self,input):\n",
    "        #Step 1: Pass the input through the encoder to get latent representation\n",
    "        pass\n",
    "    \n",
    "    def __init_weights(self,m):\n",
    "        #Init the weights (optional)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set training parameters (batch size, learning rate, optimizer, number of epochs, loss function)\n",
    "# Step 2: Create dataset (Randomly generated)\n",
    "# Step 3: Create data loader \n",
    "# Step 4: Define the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vannila Generative adversarial network (GAN)\n",
    "\n",
    "![caption](https://www.researchgate.net/profile/Zhaoqing-Pan/publication/331756737/figure/fig1/AS:736526694621184@1552613056409/The-architecture-of-generative-adversarial-networks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset \n",
    "\n",
    "For this lesson we will use SVHN dataset which readily available in `torchvision` and we will do minimal transformation operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "99446fed60054507a3816607c57ce5b6",
      "f43e4af85ed846be8d54f7ce0951ff1f",
      "c6025291f61748fc9513e56c6d6c9fa0",
      "8bfcb58458754f129976fc7ca4cbed03",
      "79489983337548eb97d6e1c21e24b532",
      "d07ad6972e9049fd8e5ef3d5cd5f3f90",
      "f67eb6d205424dbaa12a510694036317",
      "d131b597604249fbaefa428aae097039"
     ]
    },
    "colab_type": "code",
    "id": "AgVjuBIo9YcY",
    "outputId": "8cf17d50-317f-4608-f12b-c85aa90f0aff"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def normalize(data_tensor):\n",
    "    '''re-scale image values to [-1, 1]'''\n",
    "    return (data_tensor / 255.) * 2. - 1. \n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: normalize(x))])\n",
    "\n",
    "# SVHN training datasets\n",
    "svhn_train = datasets.SVHN(root='data/', split='train', download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 0\n",
    "\n",
    "# build DataLoaders for SVHN dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=svhn_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Generator & Discriminator Definition\n",
    "\n",
    "There are a couple of ways to increase the input of the generator (*z*) to the desired output size.\n",
    "1. Number of neurones\n",
    "2. Transposed Convolutions `torch.nn.ConvTranspose2d` [More info](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_dim=32):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: pass the input (real or fake samples) through all hidden layers\n",
    "        pass\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_size, conv_dim=32):\n",
    "        super(Generator, self).__init__()\n",
    "        # Step 1: Define the generator network architecture\n",
    "        # NOTE: the input is the random noise size and output is conv_dim i.e (3,32,32)\n",
    "        self.model = nn.Sequential(...) \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: pass the input which is random noise to generate the face samples\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Set hyperparams and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparams\n",
    "conv_dim = 32\n",
    "z_size = 100\n",
    "num_epochs = 50\n",
    "\n",
    "# define discriminator and generator\n",
    "D = Discriminator(conv_dim).to(device)\n",
    "G = Generator(z_size=z_size, conv_dim=conv_dim).to(device)\n",
    "\n",
    "#print the models summary \n",
    "print(D)\n",
    "print()\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Define the loss function for D(x) and G(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def real_loss(D_out, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels\n",
    "        labels = torch.FloatTensor(batch_size).uniform_(0.9, 1).to(device)\n",
    "    else:\n",
    "        labels = torch.ones(batch_size).to(device) # real labels = 1     \n",
    "    \n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.FloatTensor(batch_size).uniform_(0, 0.1).to(device) # fake labels approx 0\n",
    "    labels = labels.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "# params\n",
    "learning_rate = 0.0003\n",
    "beta1=0.5\n",
    "beta2=0.999 # default value\n",
    "\n",
    "# Create optimizers for the discriminator and generator\n",
    "d_optimizer = None\n",
    "g_optimizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 GAN training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# keep track of loss and generated, \"fake\" samples\n",
    "losses = []\n",
    "\n",
    "print_every = 2\n",
    "\n",
    "# Get some fixed data for sampling. These are images that are held\n",
    "# constant throughout training, and allow us to inspect the model's performance\n",
    "sample_size=16\n",
    "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
    "fixed_z = torch.from_numpy(fixed_z).float()\n",
    "\n",
    "# train the network\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch_i, (real_images, _) in enumerate(train_loader):\n",
    "                \n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        \n",
    "        # TRAIN THE DISCRIMINATOR\n",
    "        # Step 1: Zero gradients (zero_grad)\n",
    "        # Step 2: Train with real images\n",
    "        # Step 3: Compute the discriminator losses on real images \n",
    "        \n",
    "        D_real = None\n",
    "        d_real_loss = real_loss(D_real)\n",
    "        \n",
    "        # Step 4: Train with fake images\n",
    "        # Step 5: Generate fake images and move x to GPU, if available\n",
    "        # Step 6: Compute the discriminator losses on fake images \n",
    "        # Step 7: add up loss and perform backprop\n",
    "        \n",
    "        fake_images = None     \n",
    "        \n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        \n",
    "        #TRAIN THE GENERATOR (Train with fake images and flipped labels)\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        # Step 1: Zero gradients  \n",
    "        # Step 2: Generate fake images from random noise (z)\n",
    "        # Step 3: Compute the discriminator losses on fake images using flipped labels!\n",
    "        # Step 4: Perform backprop and take optimizer step\n",
    "\n",
    "    # Print some loss stats\n",
    "    if epoch % print_every == 0:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQsNSLUy-sbV"
   },
   "source": [
    "Keep in mind:\n",
    "\n",
    "1. Always use a learning rate for discriminator higher than the generator.\n",
    "\n",
    "2. Keep training even if you see that the losses are going up.\n",
    "\n",
    "3. There are many variations with different loss functions which are worth exploring.\n",
    "\n",
    "4. If you get mode collapse, lower the learning rates.\n",
    "\n",
    "5. Adding noise to the training data helps make the model more stable.\n",
    "\n",
    "6. Label Smoothing: instead of making the labels as 1 make it 0.9 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_abUCWlX8geC"
   },
   "source": [
    "## References\n",
    "1. [Understanding Variational Autoencoders (VAEs)](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)\n",
    "\n",
    "2. [Intuitively Understanding Variational Autoencoders](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)\n",
    "\n",
    "3. [Tutorial - What is a variational autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)\n",
    "\n",
    "4. [The variational auto-encoder](https://ermongroup.github.io/cs228-notes/extras/vae/)\n",
    "\n",
    "5. [An Introduction to Variational Autoencoders](https://arxiv.org/abs/1906.02691)\n",
    "\n",
    "6. [Basic VAE Example](https://github.com/pytorch/examples/tree/master/vae)\n",
    "\n",
    "7. [Deep Convolutional Generative Adversarial Network](https://www.tensorflow.org/tutorials/generative/dcgan)\n",
    "\n",
    "8. [Generative adversarial networks: What GANs are and how they’ve evolved](https://venturebeat.com/2019/12/26/gan-generative-adversarial-network-explainer-ai-machine-learning/)\n",
    "\n",
    "9. [Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)\n",
    "\n",
    "10. [GANs by google](https://developers.google.com/machine-learning/gan)\n",
    "\n",
    "11. [A Gentle Introduction to Generative Adversarial Networks (GANs)](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/)\n",
    "\n",
    "12. [A Beginner's Guide to Generative Adversarial Networks (GANs)](https://pathmind.com/wiki/generative-adversarial-network-gan)\n",
    "\n",
    "13. [Understanding Generative Adversarial Networks (GANs)](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29)\n",
    "\n",
    "14. [Deep Learning (PyTorch)](https://github.com/udacity/deep-learning-v2-pytorch)\n",
    "\n",
    "15. [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AML_Lab9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "79489983337548eb97d6e1c21e24b532": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8bfcb58458754f129976fc7ca4cbed03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d131b597604249fbaefa428aae097039",
      "placeholder": "​",
      "style": "IPY_MODEL_f67eb6d205424dbaa12a510694036317",
      "value": "182042624it [00:05, 30969434.06it/s]"
     }
    },
    "99446fed60054507a3816607c57ce5b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c6025291f61748fc9513e56c6d6c9fa0",
       "IPY_MODEL_8bfcb58458754f129976fc7ca4cbed03"
      ],
      "layout": "IPY_MODEL_f43e4af85ed846be8d54f7ce0951ff1f"
     }
    },
    "c6025291f61748fc9513e56c6d6c9fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d07ad6972e9049fd8e5ef3d5cd5f3f90",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_79489983337548eb97d6e1c21e24b532",
      "value": 1
     }
    },
    "d07ad6972e9049fd8e5ef3d5cd5f3f90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d131b597604249fbaefa428aae097039": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f43e4af85ed846be8d54f7ce0951ff1f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f67eb6d205424dbaa12a510694036317": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
