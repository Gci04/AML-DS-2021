{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Analysis\n",
    "```\n",
    "- Advanced Machine Learning, Innopolis University \n",
    "- Professor: Muhammad Fahim \n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "    1. Background\n",
    "    2. Dataset\n",
    "    2. Stationarity\n",
    "    3. Trend\n",
    "    4. Seasonality\n",
    "    5. ARIMA\n",
    "\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "\n",
    "1. Why & where Time Series is used?\n",
    "2. What are the components of Time Series? - (Seasonality, Trend, Noise, Cyclicity)\n",
    "3. What is Stationarity? over different time periods\n",
    "    * constant mean.\n",
    "    * constant variance or standard deviation.\n",
    "    * Auto-covariance should not depend on time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhu6Re3nKWOz"
   },
   "source": [
    "## 1.2 Dataset\n",
    "\n",
    "The data is about the number of passengers in an Airline. We want to forecast the number of passengers in the future. The data contains a particular month and number of passengers travelling in that month. The data type here is object (month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reEufHz_fYkm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "aKC02MgYfYks",
    "outputId": "e49090fa-4e5b-4e78-9544-35daf1a5f3ed"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('AirPassengers.csv')\n",
    "\n",
    "from datetime import datetime\n",
    "con=data['Month']\n",
    "data['Month']=pd.to_datetime(data['Month'])\n",
    "data.set_index('Month', inplace=True)\n",
    "#check datatype of index\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "S2m4O0xrfYk0",
    "outputId": "75c2ca03-7d8d-435a-a53e-b5bcc74da351"
   },
   "outputs": [],
   "source": [
    "#convert to time series:\n",
    "ts = data['#Passengers']\n",
    "ts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psX2BPzAI2rw"
   },
   "source": [
    "## 2. Stationarity\n",
    "\n",
    "Stationarity is an important characteristic of time series. A time series is said to be stationary if its statistical properties do not change over time. In other words, it has constant mean and variance, and covariance is independent of time.\n",
    "\n",
    "\n",
    "**Dickey-fuller Test** :This is one of the statistical tests for checking stationarity. First we consider the null hypothesis: the time series is non-stationary. The result from the rest will contain the test statistic and critical value for different confidence levels. The idea is to have Test statistics less than critical value, in this case we can reject the null hypothesis and say that this Time series is indeed stationary.\n",
    "\n",
    "\n",
    "**Why do we care about it?**\n",
    "\n",
    "1. The more predictable and not changing the data is the easier we can predict it.\n",
    "\n",
    "If your data is stationary then it should be an easy job to build a good model for predicting it, but in most cases, data will never be stationary.\n",
    "We test for stationarity by checking the values of Rolling mean and Rolling std. or by using Dickey fuller Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "_TpPWRJQfYlF",
    "outputId": "7482fdd8-939c-42bb-cbbc-c0632d7b08cc"
   },
   "outputs": [],
   "source": [
    "plt.plot(ts)\n",
    "plt.title(\"Trend Plot\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of air passengers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvH9g9d9fYlI"
   },
   "source": [
    "It’s clear from the plot that there is an overall increase in the trend and with some seasonality in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjls47jefYlI"
   },
   "source": [
    "## 2.1 Stationarity testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6lpOiAhfYlJ"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(timeseries):\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    rolmean = None\n",
    "    rolstd = None\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.xlabel(\"Time (year)\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "BVTuT7LZfYlL",
    "outputId": "4faea2b2-4461-4298-bbeb-b5cef396a973"
   },
   "outputs": [],
   "source": [
    "test_stationarity(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpX0Bs0TMLGI"
   },
   "source": [
    "This is not stationary because :\n",
    "\n",
    "* mean is increasing even though the std is small.\n",
    "* Test stat is > critical value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX6hwD8NfYlP"
   },
   "source": [
    "There are two major factors that make a time series non-stationary. \n",
    "\n",
    "They are:\n",
    "1. Trend: non-constant mean\n",
    "2. Seasonality: Variation at specific time-frames\n",
    "\n",
    "The basic idea is to model the trend and seasonality in this series, so we can remove it and make the series stationary. Then we can go ahead and apply statistical forecasting to the stationary series. And finally we can convert the forecasted values into original by applying the trend and seasonality constrains back to those that we previously separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPeJdszTfYlQ"
   },
   "source": [
    "## 3. TREND "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-hmztxSNLS4"
   },
   "source": [
    "The first step is to reduce the trend using transformation, as we can see here that there is a strong positive trend. \n",
    "\n",
    "These transformation can be log, sq-rt, cube root etc . \n",
    "\n",
    "Basically it penalizes larger values more than the smaller. \n",
    "In this case we will use the logarithmic transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "kpPZVBHOfYlQ",
    "outputId": "d3dfa2cf-40db-40f3-acdd-bf79bb8d5cd3"
   },
   "outputs": [],
   "source": [
    "ts_log = np.log(ts)\n",
    "plt.plot(ts_log)\n",
    "plt.title(\"Trend Plot\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"# of passengers \\n (Log scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MpoY6oaNnoz"
   },
   "source": [
    "Check the y axis it is definetly less intense but still the trend is there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2SH_Vq3fYlS"
   },
   "source": [
    "## 3.1 Smoothing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl7uCCv9N1LW"
   },
   "source": [
    "In smoothing we usually take the past few instances (rolling estimates) We will discuss two methods under smoothing- Moving average and Exponentially weighted moving average.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mQINivCfYlT"
   },
   "source": [
    "## 3.2 Moving average\n",
    "First take $x$ consecutive values and this depends on the frequency if it is 1 year we take 12 values. Lucky for us that Pandas has a function for rolling estimate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "IUSH5rTifYlU",
    "outputId": "542ea3e8-9c90-4c2d-be31-b5764d949971"
   },
   "outputs": [],
   "source": [
    "moving_avg = ts_log.rolling(12).mean()\n",
    "plt.plot(ts_log)\n",
    "plt.plot(moving_avg, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "X9g43X6-fYlW",
    "outputId": "36e72a84-ec66-4168-cb26-5e9f02803e0e"
   },
   "outputs": [],
   "source": [
    "ts_log_moving_avg_diff = ts_log - moving_avg\n",
    "ts_log_moving_avg_diff.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5t8RThnOEIq"
   },
   "source": [
    "The reason there are null values is because we take the average of first 12 so 11 values are null. We can also see that in the visual representation. Thus it is dropped for further analysis. Now let’s parse it to the function to check for stationarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "s1sgFIf3fYlZ",
    "outputId": "368159b9-9135-4b1c-b57f-27e221dc285a"
   },
   "outputs": [],
   "source": [
    "ts_log_moving_avg_diff.dropna(inplace=True)\n",
    "ts_log_moving_avg_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "Nf3iaMgEfYlb",
    "outputId": "6bc64b1e-fefc-4409-a024-7b2d3e1f73b8"
   },
   "outputs": [],
   "source": [
    "test_stationarity(ts_log_moving_avg_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6mT8p5MOI6F"
   },
   "source": [
    "We notice two things:\n",
    "\n",
    "* The rolling values are varying slightly but there is no specific trend.\n",
    "* The test statistics is smaller than the 5 % critical values. That tells us that we are 95% confident that this series is stationary.\n",
    "\n",
    "\n",
    "In this example we can easily take a time period (12 months for a year), but there are situations where the time period range is more complex like stock price etc. So we use the exponentially weighted moving average (there are other weighted moving averages but for starters, lets use this). The previous values are assigned with a decay factor. Pandas again comes to the rescue with some awesome functions for it, like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNTFvD1ofYle"
   },
   "source": [
    "## 3.3 Exponentially weighted moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "tEEMoe8kfYle",
    "outputId": "2ece4618-31ea-47b4-810b-f31fc68c5145"
   },
   "outputs": [],
   "source": [
    "expwighted_avg = ts_log.ewm(halflife=12).mean()\n",
    "plt.plot(ts_log)\n",
    "plt.plot(expwighted_avg, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "PbDPlvhDfYlg",
    "outputId": "216fdc3d-7bc2-41cb-c370-d91f5f78e830"
   },
   "outputs": [],
   "source": [
    "ts_log_ewma_diff = ts_log - expwighted_avg\n",
    "test_stationarity(ts_log_ewma_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__6b9GatOR4d"
   },
   "source": [
    "It is stationary because:\n",
    "* Rolling values have less variations in mean and standard deviation in magnitude.\n",
    "* the test statistic is smaller than 1% of the critical value. So we can say we are almost 99% confident that this is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJAYDFqufYlj"
   },
   "source": [
    "## 4. SEASONALITY (ALONG WITH TREND) \n",
    "\n",
    "Previously we saw just trend part of the time series, now we will see both trend and seasonality. \n",
    "\n",
    "Most Time series have trends along with seasonality. There are two common methods to remove trend and seasonality, they are:\n",
    "1. Differencing: by taking difference using time lag\n",
    "2. Decomposition: model both trend and seasonality, then remove them\n",
    "\n",
    "## 4.1 Differencing\n",
    "\n",
    "Here we first take the difference of the value at a particular time with that of the previous time. Now let’s do it in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "OqUvZCZzfYlj",
    "outputId": "8b17dde6-5793-450c-e509-66abb0fe7442"
   },
   "outputs": [],
   "source": [
    "#Take first difference:\n",
    "ts_log_diff = ts_log - ts_log.shift(1)\n",
    "plt.plot(ts_log_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "PexIEs3VfYlo",
    "outputId": "ec5be961-41e4-419a-bca5-1fe9671dbdbf"
   },
   "outputs": [],
   "source": [
    "ts_log_diff.dropna(inplace=True)\n",
    "test_stationarity(ts_log_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Om4o8desPQHO"
   },
   "source": [
    "It is stationary because:\n",
    "\n",
    "* the mean and std variations have small variations with time.\n",
    "* test statistic is less than 10% of the critical values, so we can be 90 % confident that this is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AaxRdRSPS5f"
   },
   "source": [
    "## 4.2 Decomposing: \n",
    "\n",
    "Here we model both the trend and the seasonality, then the remaining part of the time series is returned. Guess what? Yup, we have some awesome function for it. Let’s check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "S6uWJKjjfYlq",
    "outputId": "a3a9277a-5d79-4bdb-fd5e-d6cf1f92c797"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(ts_log)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(ts_log, label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajtVcAW4Pc6n"
   },
   "source": [
    "Remove the trend and seasonality from the Time series and now we can use the residual values. Let’s check stationarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "jaBDQ1V1fYls",
    "outputId": "314a9e90-bf5d-4946-aac8-0c892b6d20a0"
   },
   "outputs": [],
   "source": [
    "ts_log_decompose = residual\n",
    "ts_log_decompose.dropna(inplace=True)\n",
    "test_stationarity(ts_log_decompose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn5MFUUHPnOG"
   },
   "source": [
    "This is stationary because:\n",
    "1. test statistic is lower than 1% critical values.\n",
    "2. the mean and std variations have small variations with time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74dbWdOAfYlu"
   },
   "source": [
    "## 5. FORECASTING A TIME SERIES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbZAR6S1P2-y"
   },
   "source": [
    "Now that we have made the Time series stationary, let’s make models on the time series using differencing because it is easy to add the error , trend and seasonality back into predicted values .\n",
    "\n",
    "We will use statistical modelling method called ARIMA to forecast the data where there are dependencies in the values.\n",
    "\n",
    "Auto Regressive Integrated Moving Average(ARIMA) — It is like a liner regression equation where the predictors depend on parameters (p,d,q) of the ARIMA model.\n",
    "\n",
    "* p : This is the number of AR (Auto-Regressive) terms. Example — if p is 3 the predictor for y(t) will be y(t-1),y(t-2),y(t-3).\n",
    "\n",
    "* q : This is the number of MA (Moving-Average) terms.\n",
    "\n",
    "* d :This is the number of differences or the number of non-seasonal differences .\n",
    "\n",
    "Now let’s check out on how we can figure out what value of p and q to use. We use two popular plotting techniques; they are:\n",
    "\n",
    "1. Autocorrelation Function (ACF): It just measures the correlation between two consecutive (lagged version). example at lag 4, ACF will compare series at time instance t1…t2 with series at instance t1–4…t2–4\n",
    "\n",
    "2. Partial Autocorrelation Function (PACF): is used to measure the degree of association between y(t) and y(t-p)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YfmmTx5fYlv"
   },
   "source": [
    "## 5.1 ACF & PACF \n",
    "\n",
    "ACF explains how the present value of a given time series is correlated with the past values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "dBfHQCNsfYlx",
    "outputId": "e1de66ce-6c2e-4002-f774-43def2318053"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "#ACF and PACF plots:\n",
    "from statsmodels.tsa.stattools import acf, pacf  \n",
    "\n",
    "lag_acf = acf(ts_log_diff, nlags=12)\n",
    "lag_pacf = pacf(ts_log_diff, nlags=12, method='ols')\n",
    "\n",
    "#Plot ACF:    \n",
    "plt.subplot(121)    \n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.title('Autocorrelation Function')\n",
    "\n",
    "#Plot PACF:\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gsm8xtLgQwlD"
   },
   "source": [
    "The two dotted lines on either sides of 0 are the confidence intervals. \n",
    "These can be used to determine the ‘p’ and ‘q’ values as:\n",
    "\n",
    "* p: The first time where the PACF crosses the upper confidence interval, here its close to 2. hence p = 2.\n",
    "* q: The first time where the ACF crosses the upper confidence interval, here its close to 2. hence p = 2.\n",
    "\n",
    "Now, using this make 3 different ARIMA models considering individual as well as combined effects. I will also print the RSS for each.\n",
    "\n",
    "The “residuals” in a time series model are what is left over after fitting a model. The residuals are equal to the difference between the observations and the corresponding fitted values:\n",
    "\n",
    "Please note that here RSS is for the values of residuals and not actual series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "cAqij7NFfYlz",
    "outputId": "c48faffb-7f83-4bd8-b2d7-7eca50e2e729"
   },
   "outputs": [],
   "source": [
    "model = ARIMA(ts_log, order=(2,1,0))\n",
    "results_AR = model.fit(disp=-1)\n",
    "plt.plot(ts_log_diff)\n",
    "plt.plot(results_AR.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((results_AR.fittedvalues - ts_log_diff)**2))\n",
    "plt.xlabel(\"Time (year)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "aKGvdParRqJ7",
    "outputId": "0236c071-db40-4168-f7d3-f9cd9301e508"
   },
   "outputs": [],
   "source": [
    "model = ARIMA(ts_log, order=(0,1,2))\n",
    "results_MA = model.fit(disp=-1)\n",
    "plt.plot(ts_log_diff)\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((results_MA.fittedvalues - ts_log_diff)**2))\n",
    "plt.xlabel(\"Time (year)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "jLbQqXwRRyD5",
    "outputId": "f111025a-262a-4e49-96ac-97bb7b1ff14c"
   },
   "outputs": [],
   "source": [
    "model = ARIMA(ts_log, order=(2,1,2))\n",
    "results_ARIMA = model.fit(disp=-1)\n",
    "plt.plot(ts_log_diff)\n",
    "plt.plot(results_ARIMA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - ts_log_diff)**2))\n",
    "plt.xlabel(\"Time (year)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_R72AMBR6ax"
   },
   "source": [
    "RSS values:\n",
    "* AR=1.5023\n",
    "* MA=1.472\n",
    "* ARIMA =1.0292\n",
    "\n",
    "ARIMA has the best RSS values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p41m8M5ZR9qC"
   },
   "source": [
    "## 5.3 FINAL STEP: BRINGING THIS BACK TO THE ORIGINAL SCALE\n",
    "\n",
    "Steps involved:\n",
    "\n",
    "• First get the predicted values and store it as series. You will notice the first month is missing because we took a lag of 1(shift).\n",
    "\n",
    "• Now convert differencing to log scale: find the cumulative sum and add it to a new series with a base value (here the first-month value of the log series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "T51T16TjSDUB",
    "outputId": "42ea0157-1da6-4d03-a5a0-4fcc813070c5"
   },
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues,copy=True)\n",
    "predictions_ARIMA_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "n48FDe5XSTgS",
    "outputId": "6e9df9b9-4ce6-42ef-9f63-7933aa48adbe"
   },
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff_cum_sum = predictions_ARIMA_diff.cumsum()\n",
    "predictions_ARIMA_diff_cum_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "bbDXR-5ASe--",
    "outputId": "3cf2474e-3420-4454-b2c6-347925955d61"
   },
   "outputs": [],
   "source": [
    "predictions_ARIMA_log = pd.Series(ts_log.iloc[0],index=ts_log.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cum_sum,fill_value=0)\n",
    "predictions_ARIMA_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UPtMvsvS9UJ"
   },
   "source": [
    "• Next -take the exponent of the series from above (anti-log) which will be the predicted value — the time series forecast model.\n",
    "Now plot the predicted values with the original.\n",
    "• Find the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "87qikEs9S-nq",
    "outputId": "31fb73de-d618-4d05-c400-ad2083bac5fb"
   },
   "outputs": [],
   "source": [
    "predictions_ARIMA = np.exp(predictions_ARIMA_log)\n",
    "plt.plot(ts)\n",
    "plt.plot(predictions_ARIMA)\n",
    "plt.title('RMSE: %.4f'%np.sqrt(sum((predictions_ARIMA-ts)**2)))\n",
    "plt.ylabel(\"# of passengers\")\n",
    "plt.xlabel(\"Time (year)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8hwtaz64UZQ"
   },
   "source": [
    "## References\n",
    "\n",
    "1. [The Complete Guide to Time Series Analysis and Forecasting](https://towardsdatascience.com/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775)\n",
    "\n",
    "2. [statsmodels Documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMA.html)\n",
    "\n",
    "3. [Time Series Forecast : A basic introduction using Python.](https://medium.com/@stallonejacob/time-series-forecast-a-basic-introduction-using-python-414fcb963000)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "ML LAB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
