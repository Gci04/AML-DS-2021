{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AML_Lab5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quarriedstone/AML-DS-2021/blob/main/AML_Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_MZyTNWqOch"
      },
      "source": [
        "### Week 3: Hidden Markov Models\n",
        "```\n",
        "- Advanced Machine Learning, Innopolis University \n",
        "- Professor: Muhammad Fahim \n",
        "- Teaching Assistant: Gcinizwe Dlamini\n",
        "```\n",
        "<hr>\n",
        "\n",
        "```\n",
        "Lab Plan\n",
        "    1. Homework 1 Discussion\n",
        "    2. HMM for POS tagging \n",
        "    3. Hidden Markov Models\n",
        "    4. Manual Calculations\n",
        "    5. CG rich region identification\n",
        "```\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWjAvNk9V4q8"
      },
      "source": [
        "**What's the probability that a random day is sunny?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rpHexQzBflS"
      },
      "source": [
        "**if bob is happy, what's the probability that it's sunny?**\n",
        "\n",
        "*hint: use Bayes rule.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvtWfH_9z3S9",
        "outputId": "f2f1bd66-730c-4b2d-894b-d3488502ec5e"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "states = ('Sunny', 'Rainy')\n",
        "observations = ('happy', 'grumpy')\n",
        "pi = np.array([2./3., 1./3.])  #initial probability \n",
        "A = np.array([[7./9., 2./9.],[0.4, 0.6]]) #Transmission probability \n",
        "B = np.array([[0.8, 0.2],[0.4, 0.6]]) #Emission probability\n",
        "bob_says = np.array([0,0,1,1,1,0])\n",
        "\n",
        "def forward(obs_seq, pi, A, B):\n",
        "    T = len(obs_seq)\n",
        "    N = A.shape[0]\n",
        "    alpha = np.zeros((T, N))\n",
        "    alpha[0] = pi*B[:,obs_seq[0]]\n",
        "    for t in range(1, T):\n",
        "        alpha[t] = np.inner(alpha[t-1],A) * B[:, obs_seq[t]]\n",
        "    return alpha\n",
        "\n",
        "def likelihood(alpha):\n",
        "    # returns log P(Y  \\mid  model)\n",
        "    # using the forward part of the forward-backward algorithm\n",
        "    return  alpha[-1].sum()  \n",
        "\n",
        "alpha = forward(bob_says, pi, A, B)\n",
        "print(alpha)\n",
        "\n",
        "print(likelihood(alpha))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.53333333 0.13333333]\n",
            " [0.35555556 0.11733333]\n",
            " [0.06052346 0.12757333]\n",
            " [0.01508469 0.06045203]\n",
            " [0.00503326 0.02538306]\n",
            " [0.00764435 0.00689726]]\n",
            "0.014541607035957256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8u7V-vzB0UQ"
      },
      "source": [
        "### **Viterbi Algorithm**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oNjHwNQV38A",
        "outputId": "54a71422-de39-4d7b-85e3-c81611d65804"
      },
      "source": [
        "from numpy import random\n",
        "# Transition Probabilities\n",
        "p_ss = 7./9.\n",
        "p_sr = 2./9.\n",
        "p_rs = 0.4\n",
        "p_rr = 0.6\n",
        "\n",
        "# Initial Probabilities\n",
        "p_s = 2/3\n",
        "p_r = 1/3\n",
        "\n",
        "# Emission Probabilities\n",
        "p_sh = 0.8\n",
        "p_sg = 0.2\n",
        "p_rh = 0.4\n",
        "p_rg = 0.6\n",
        "\n",
        "moods = ['H', 'H', 'G', 'G', 'G', 'H']\n",
        "probabilities = []\n",
        "weather = []\n",
        "\n",
        "if moods[0] == 'H':\n",
        "    probabilities.append((p_s*p_sh, p_r*p_rh))\n",
        "else:\n",
        "    probabilities.append((p_s*p_sg, p_r*p_rg))\n",
        "\n",
        "for i in range(1,len(moods)):\n",
        "    yesterday_sunny, yesterday_rainy = probabilities[-1]\n",
        "    if moods[i] == 'H':\n",
        "        today_sunny = max(yesterday_sunny*p_ss*p_sh, yesterday_rainy*p_rs*p_sh)\n",
        "        today_rainy = max(yesterday_sunny*p_sr*p_rh, yesterday_rainy*p_rr*p_rh)\n",
        "        probabilities.append((today_sunny, today_rainy))\n",
        "    else:\n",
        "        today_sunny = max(yesterday_sunny*p_ss*p_sg, yesterday_rainy*p_rs*p_sg)\n",
        "        today_rainy = max(yesterday_sunny*p_sr*p_rg, yesterday_rainy*p_rr*p_rg)\n",
        "        probabilities.append((today_sunny, today_rainy))\n",
        "\n",
        "for p in probabilities:\n",
        "    if p[0] > p[1]:\n",
        "        weather.append('S')\n",
        "    else:\n",
        "        weather.append('R')\n",
        "        \n",
        "weather"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['S', 'S', 'S', 'R', 'R', 'S']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTblFdwJWP2e",
        "outputId": "207b3bf8-92f2-4faa-aa38-ac8c8d07fcae"
      },
      "source": [
        "probabilities"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.5333333333333333, 0.13333333333333333),\n",
              " (0.33185185185185184, 0.047407407407407405),\n",
              " (0.051621399176954734, 0.044246913580246905),\n",
              " (0.008029995427526292, 0.015928888888888885),\n",
              " (0.001274311111111111, 0.005734399999999998),\n",
              " (0.0018350079999999995, 0.0013762559999999995)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdxphM0SqOcq"
      },
      "source": [
        "### Is there any Python package for all these computation? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0yDBYvZqOcr"
      },
      "source": [
        "![](https://cdn.pixabay.com/photo/2018/03/25/11/43/pomegranate-3259161_960_720.jpg)\n",
        "\n",
        "Hidden Markov models (HMMs) are the flagship of the pomegranate package in that they have the most features of all of the models and that they were the first algorithm implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktgdNE_MkrpB"
      },
      "source": [
        "## CG rich region identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ-jW22pkq0q"
      },
      "source": [
        "\n",
        "Hidden Markov models are a form of structured prediction method that are popular for tagging all elements in a sequence with some \"hidden\" state. They can be thought of as extensions of Markov chains where, instead of the probability of the next observation being dependant on the current observation, the probability of the next hidden state is dependant on the current hidden state, and the next observation is derived from that hidden state. An example of this can be part of speech tagging, where the observations are words and the hidden states are parts of speech. Each word gets tagged with a part of speech, but dynamic programming is utilized to search through all potential word-tag combinations to identify the best set of tags across the entire sentence.\n",
        "\n",
        "Another perspective of HMMs is that they are an extension on mixture models that includes a transition matrix. Conceptually, a mixture model has a set of \"hidden\" states---the mixture components---and one can calculate the probability that each sample belongs to each component. This approach treats each observations independently. However, like in the part-of-speech example we know that an adjective typically is followed by a noun, and so position in the sequence matters. A HMM adds a transition matrix between the hidden states to incorporate this information across the sequence, allowing for higher probabilities of transitioning from the \"adjective\" hidden state to a noun or verb.\n",
        "\n",
        "pomegranate implements HMMs in a flexible manner that goes beyond what other packages allow. Let's see some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "o1ghUA5Lk3Hq",
        "outputId": "a46c3368-35cc-406c-ad71-4b2635a4e212"
      },
      "source": [
        "!pip3 install pomegranate"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pomegranate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/65/98bbcc034daac7b584e82a0f91f140dbb9ef502a1b7c96f16f3deb847232/pomegranate-0.14.4-cp37-cp37m-manylinux2010_x86_64.whl (17.9MB)\n",
            "\u001b[K     |████████████████████████████████| 17.9MB 203kB/s \n",
            "\u001b[?25hCollecting numpy>=1.20.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/ef/8967d406f3f85018ceb5efab50431e901683188f1741ceb053efcab26c87/numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 210kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.9.0b4 in /usr/local/lib/python3.7/dist-packages (from pomegranate) (1.0.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from pomegranate) (3.13)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pomegranate) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from pomegranate) (2.5.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->pomegranate) (4.4.2)\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, pomegranate\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "Successfully installed numpy-1.20.2 pomegranate-0.14.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBazN7fukxCI"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn; seaborn.set_style('whitegrid')\n",
        "import numpy\n",
        "\n",
        "from pomegranate import *\n",
        "\n",
        "numpy.random.seed(0)\n",
        "numpy.set_printoptions(suppress=True)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H8ZDws1k9QI"
      },
      "source": [
        "**CG rich region identification example**\n",
        "Lets take the simplified example of CG island detection on a sequence of DNA. DNA is made up of the four canonical nucleotides, abbreviated 'A', 'C', 'G', and 'T'. We can say that regions of the genome that are enriched for nucleotides 'C' and 'G' are 'CG islands', which is a simplification of the real biological concept but sufficient for our example. The issue with identifying these regions is that they are not exclusively made up of the nucleotides 'C' and 'G', but have some 'A's and 'T's scatted amongst them. A simple model that looked for long stretches of C's and G's would not perform well, because it would miss most of the real regions.\n",
        "\n",
        "We can start off by building the model. Because HMMs involve the transition matrix, which is often represented using a graph over the hidden states, building them requires a few more steps that a simple distribution or the mixture model. Our simple model will be composed of two distributions. One distribution wil be a uniform distribution across all four characters and one will have a preference for the nucleotides C and G, while still allowing the nucleotides A and T to be present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_07NhYDk_SZ"
      },
      "source": [
        "d1 = DiscreteDistribution({'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25})\n",
        "d2 = DiscreteDistribution({'A': 0.10, 'C': 0.40, 'G': 0.40, 'T': 0.10})\n",
        "\n",
        "# For the HMM we have to first define states, which are a pair of a distribution and a name.\n",
        "s1 = State(d1, name='background')\n",
        "s2 = State(d2, name='CG island')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK-bq9cIlGdR"
      },
      "source": [
        "# Now we define the HMM and pass in the states.\n",
        "model = HiddenMarkovModel()\n",
        "model.add_states(s1, s2)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZJEDKhPlNy2"
      },
      "source": [
        "Then we have to define the transition matrix, which is the probability of going from one hidden state to the next hidden state. In some cases, like this one, there are high self-loop probabilities, indicating that it's likely that one will stay in the same hidden state from one observation to the next in the sequence. Other cases have a lower probability of staying in the same state, like the part of speech tagger. A part of the transition matrix is the start probabilities, which is the probability of starting in each of the hidden states. Because we create these transitions one at a time, they are very amenable to sparse transition matrices, where it is impossible to transition from one hidden state to the next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-naWxt2lLbG"
      },
      "source": [
        "model.add_transition(model.start, s1, 0.5)\n",
        "model.add_transition(model.start, s2, 0.5)\n",
        "model.add_transition(s1, s1, 0.9)\n",
        "model.add_transition(s1, s2, 0.1)\n",
        "model.add_transition(s2, s1, 0.1)\n",
        "model.add_transition(s2, s2, 0.9)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loY5Jz8NlcZL"
      },
      "source": [
        "model.bake()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM6orppQleLA",
        "outputId": "333dd162-022e-4a0a-87c4-c7c8d09139d1"
      },
      "source": [
        "seq = numpy.array(list('CGACTACTGACTACTCGCCGACGCGACTGCCGTCTATACTGCGCATACGGC'))\n",
        "\n",
        "hmm_predictions = model.predict(seq)\n",
        "\n",
        "print(\"sequence: {}\".format(''.join(seq)))\n",
        "print(\"hmm pred: {}\".format(''.join(map( str, hmm_predictions))))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequence: CGACTACTGACTACTCGCCGACGCGACTGCCGTCTATACTGCGCATACGGC\n",
            "hmm pred: 111111111111111000000000000000011111111111111110000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBiAM9CelgxH"
      },
      "source": [
        "Note that all we did was add a transition from `s1` to `model.end` with some low probability. This probability doesn't have to be high if there's only a single transition there, because there's no other possible way of getting to the end state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwXo7_MelhHk",
        "outputId": "4ad7dc05-7fd1-4eb0-e6b7-d7325887a4f8"
      },
      "source": [
        "model = HiddenMarkovModel()\n",
        "model.add_states(s1, s2)\n",
        "model.add_transition(model.start, s1, 0.5)\n",
        "model.add_transition(model.start, s2, 0.5)\n",
        "model.add_transition(s1, s1, 0.89 )\n",
        "model.add_transition(s1, s2, 0.10 )\n",
        "model.add_transition(s1, model.end, 0.01)\n",
        "model.add_transition(s2, s1, 0.1 )\n",
        "model.add_transition(s2, s2, 0.9)\n",
        "model.bake()\n",
        "\n",
        "seq = numpy.array(list('CGACTACTGACTACTCGCCGACGCGACTGCCGTCTATACTGCGCATACGGC'))\n",
        "\n",
        "hmm_predictions = model.predict(seq)\n",
        "\n",
        "print(\"sequence: {}\".format(''.join(seq)))\n",
        "print(\"hmm pred: {}\".format(''.join(map( str, hmm_predictions))))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequence: CGACTACTGACTACTCGCCGACGCGACTGCCGTCTATACTGCGCATACGGC\n",
            "hmm pred: 111111111111111000000000000000011111111111111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yUFoZjZqOcv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}